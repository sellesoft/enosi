$ require "common"
$ local helpers = require "graphics.Helpers"

@@lpp.import "graphics/Vulkan.lh"

@@lpp.import "graphics/Buffer.lh"
@@lpp.import "graphics/Geo.lh"
@@lpp.import "graphics/Renderer.lh"

#include "iro/Logger.h"

@defFileLogger(gfx.buffer, Info)

namespace gfx
{

/* ----------------------------------------------------------------------------
 */
Buffer Buffer::create(Renderer& r, const CreateParams& params)
{
  String name = resolved(params.debug_name, "unnamed"_str);

$ local cerr = helpers.defCreateErr("gfx::Buffer", "name")

  DEBUG("creating a gfx::Buffer of size ", params.size, " named ", name, '\n');

  if (params.size == 0)
  {
    @cerr("cannot create gfx::Buffer of size 0")
    return nil;
  }

  if ((params.properties & MemoryProperty::LazilyAllocated) && 
      (params.properties & MemoryProperty::HostCoherent
                         | MemoryProperty::HostVisible))
  {
    @cerr("memory property LazilyAllocated is incompatible with HostVisible "
          "and HostCoherent");
    return nil;
  }

  if (params.behavior == MappingBehavior::Never &&
      (params.properties & (MemoryProperty::HostVisible
                          | MemoryProperty::HostCoherent
                          | MemoryProperty::HostCached)))
  {
    @cerr("mapping behavior Never is incompatible with memory properties ",
          "HostVisible, HostCoherent, and HostCached");
    return nil;
  }

  VkBufferUsageFlags usage = 0;
  switch (params.usage)
  {
  case Usage::UniformBuffer: usage = VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT; break;
  case Usage::StorageBuffer: usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT; break;
  case Usage::IndexBuffer: usage = VK_BUFFER_USAGE_INDEX_BUFFER_BIT; break;
  case Usage::VertexBuffer: usage = VK_BUFFER_USAGE_VERTEX_BUFFER_BIT; break;
  }

  if (params.behavior == MappingBehavior::Never)
    usage |= VK_BUFFER_USAGE_TRANSFER_DST_BIT;

  VkBuffer handle;
  if (!r.vk->createVkBuffer(&handle, params.size, usage, name))
    return nil;

  auto failsafe_destroy_buffer = deferWithCancel
  {
    r.vk->destroyVkBuffer(handle);
  };

  VkMemoryPropertyFlags properties = 0;
$ local function mapprop(x, y)
  if (params.properties & MemoryProperty::$(x))
    properties |= VK_MEMORY_PROPERTY_$(y)_BIT;
$ end
  @mapprop(DeviceLocal,     DEVICE_LOCAL);
  @mapprop(HostVisible,     HOST_VISIBLE);
  @mapprop(HostCoherent,    HOST_COHERENT);
  @mapprop(HostCached,      HOST_CACHED);
  @mapprop(LazilyAllocated, LAZILY_ALLOCATED);

  VkMemoryRequirements memreq;
  vkGetBufferMemoryRequirements(r.vk->device, handle, &memreq);

  DeviceHeapAllocation* ptr = 
    r.vk->allocateAndBindVkBufferMemory(handle, properties, memreq);

  if (ptr == nullptr)
  {
    @cerr("failed to allocate and bind memory for VkBuffer");
    return nil;
  }

  auto failsafe_deallocate = deferWithCancel
  {
    r.vk->deallocate(ptr);
  };

  void* mapped_data = nullptr;

  switch (params.behavior)
  {
  case MappingBehavior::Never:
    if (params.data == nullptr)
    {
      WARN(
        "request to allocate a gfx::Buffer with mapping behavior Never. This "
        "type of request is mainly useful for intermediate compute shader "
        "memory, but compute shaders are not set up yet.\n");
    }
    else
    {
      if (!r.vk->stageVkBufferMemory(
            params.data,
            params.size,
            handle,
            memreq.size))
      {
        @cerr("failed to stage memory for gfx::Buffer");
        return nil;
      }
    }
    break;

  case MappingBehavior::Occasional:
    if (params.data != nullptr)
    {
      if (!r.vk->mapCopyAndFlushVkBufferMemory(
            params.data,
            ptr->aligned_offset,
            params.size,
            ptr))
      {
        @cerr("failed to map/copy/flush initial data for gfx::Buffer");
        return nil;
      }
    }
    break;

  case MappingBehavior::Persistent:
    {
      if (!r.vk->mapVkBuffer(
            &mapped_data, 
            ptr->aligned_offset,
            ptr))
      {
        @cerr("failed to map persistent gfx::Buffer");
        return nil;
      }

      if (params.data != nullptr)
      {
        mem::copy(mapped_data, params.data, params.size);
        r.vk->flushMappedVkBuffer(
            ptr->aligned_offset, 
            VK_WHOLE_SIZE, 
            ptr);
      }
    }
    break;
  }

  DeviceBuffer* internal_buffer = r.vk->buffer_pool.add();
  internal_buffer->handle = handle;
  internal_buffer->ptr = ptr;

  if (params.usage == Buffer::UniformBuffer)
  {
    if (!r.vk->allocateVkDescriptorSet(
          &internal_buffer->set,
          &(VkDescriptorSetLayout&)r.default_ubo_set_layout.handle,
          name))
    {
      @cerr("failed to allocate descriptor set");
      return nil;
    }

    VkDescriptorBufferInfo descriptor_info = 
    {
      .buffer = internal_buffer->handle,
      .offset = 0,
      .range = params.size,
    };

    r.vk->updateVkDescriptorSet_Buffer(
      internal_buffer->set,
      VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
      0,
      0,
      makeSlice(&descriptor_info, 1));
  }

  failsafe_destroy_buffer.cancel();
  failsafe_deallocate.cancel();

  // The handle is our internal representation instead of the VkBuffer, as 
  // we have to track its device memory internally.
  return {internal_buffer, mapped_data};
}

/* ----------------------------------------------------------------------------
 */
void Buffer::destroy(Renderer& r)
{
  if (isnil(*this))
    return;

  DEBUG("destroying ", *this, "\n");

  auto* internal_buffer = (DeviceBuffer*)handle;

  // Unmapping when this is not mapped is safe and handled in this call.
  r.vk->unmapVkBuffer(internal_buffer->ptr);
  r.vk->destroyVkBuffer(internal_buffer->handle);
  r.vk->deallocate(internal_buffer->ptr);

  r.vk->buffer_pool.remove(internal_buffer);
  handle = nullptr;
}

/* ----------------------------------------------------------------------------
 */
void* Buffer::map(Renderer& r)
{
  if (isnil(*this))
  {
    ERROR("attempt to map a nil gfx::Buffer\n");
    return nullptr;
  }

  auto* internal_buffer = (DeviceBuffer*)handle;

  TRACE("mapping ", *this, "\n");

  if (!r.vk->mapVkBuffer(
      &mapped_data, 
      internal_buffer->ptr->aligned_offset,
      internal_buffer->ptr))
    return nullptr;

  return mapped_data;
}

/* ----------------------------------------------------------------------------
 */
b8 Buffer::flush(Renderer& r, u64 offset, u64 size)
{
  if (isnil(*this))
    return ERROR("attempt to flush a nil gfx::Buffer\n");

  if (size == FLUSH_WHOLE_BUFFER)
    TRACE("flushing whole buffer of ", *this, "\n");
  else
    TRACE("flushing ", size, " bytes of " , *this, " at offset ", offset, 
          "\n");

  auto* internal_buffer = (DeviceBuffer*)handle;

  DeviceHeapAllocation* ptr = internal_buffer->ptr;

  VkDeviceSize dev_size = size;
  VkDeviceSize dev_offset = ptr->aligned_offset + offset;

  if (size == FLUSH_WHOLE_BUFFER)
    dev_size = ptr->aligned_size;

  // NOTE(sushi) size and offset are aligned inside this call.
  return r.vk->flushMappedVkBuffer(dev_offset, dev_size, internal_buffer->ptr);
}

/* ----------------------------------------------------------------------------
 */
b8 Buffer::copyAndFlush(Renderer& r, void* data, u64 size)
{
  if (isnil(*this))
    return ERROR("attempt to copy and flush a nil gfx::Buffer\n");

  TRACE("copying and flushing ", *this, "\n");

  if (mapped_data == nullptr)
    return ERROR("attempt to copy and flush unmapped ", *this, "\n");

  auto* internal_buffer = (DeviceBuffer*)handle;
  DeviceHeapAllocation* ptr = internal_buffer->ptr;

  DEBUG("copying ", size, " data\n");
  mem::copy(mapped_data, data, size);

  VkDeviceSize dev_size = size;
  VkDeviceSize dev_offset = ptr->aligned_offset;

  if (size == FLUSH_WHOLE_BUFFER)
    dev_size = VK_WHOLE_SIZE;
  else 
    dev_size = alignUp(size, r.vk->getHeap(ptr)->alignment);

  DEBUG("dsize: ", dev_size, '\n');

  return r.vk->flushMappedVkBuffer(dev_offset, dev_size, ptr);
}

/* ----------------------------------------------------------------------------
 */
void Buffer::unmap(Renderer& r)
{
  if (isnil(*this))
  {
    ERROR("attempt to unmap a nil gfx::Buffer\n");
    return;
  }

  auto* internal_buffer = (DeviceBuffer*)handle;

  TRACE("unmapping ", *this, '\n');

  if (!r.vk->unmapVkBuffer(internal_buffer->ptr))
    ERROR("attempt to unmap ", *this, " but it was already unmapped\n"); 
}

}
