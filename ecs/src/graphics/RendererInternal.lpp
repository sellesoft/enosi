$ local cmn = require "common"

// TODO(sushi) do a reflected import to handle any VkResults we aren't 
//             handling in the VkResult info func.
@@lpp.import "graphics/RendererInternal.lh"

$ if IRO_LINUX then
#define Window X11Window
#define Font X11Font
#define Time X11Time
#define KeyCode X11KeyCode
#define VK_USE_PLATFORM_XLIB_KHR
#include <vulkan/vulkan.h>
#undef Window
#undef Font
#undef Time
#undef KeyCode
#undef None
#undef X
@lpp.import "../window/Window_linux.lh"
$ elseif IRO_WIN32 then --if IRO_LINUX
#define VK_USE_PLATFORM_WIN32_KHR
#include <vulkan/vulkan.h>
#undef ERROR
#undef min
#undef max
#undef interface
@lpp.import "../window/Window_win32.lh"
$ else -- elseif IRO_WIN32
#error "unhandled platform"
$ end -- if IRO_LINUX

@@lpp.import "window/Window.lh"

#include "iro/Logger.h"
#include "iro/memory/Bump.h"
#include "iro/time/Time.h"
#include "iro/fs/File.h"

@defFileLogger(gfx, Info)

@@lpp.import "graphics/Buffer.lh"
@@lpp.import "graphics/Pipeline.defs.lh"
@@lpp.import "graphics/Geo.lh"

namespace gfx
{

$ local print_success = false
$ local print_non_error_success_as_warning = false

$ local function arrlen(arr)
  sizeof($(arr)) / sizeof($(arr)[0])
$ end

// Constant definitions of initialization params we pass to vulkan,
// centralized so they're easy to find and change.

// ****************************************************************************
static const char* c_app_name = "ecs";
static const u32 c_app_version = VK_MAKE_VERSION(0, 0, 1);
static const char* c_engine_name = c_app_name;
static const u32 c_engine_version = c_app_version;
static const u32 c_vulkan_api_version = VK_API_VERSION_1_4;

// ****************************************************************************
static const char* c_validation_layers[] = 
{
  "VK_LAYER_KHRONOS_validation",
};
static const u32 c_validation_layer_count = @arrlen(c_validation_layers);

// ****************************************************************************
static const VkValidationFeatureEnableEXT c_validation_features[] = 
{
  VK_VALIDATION_FEATURE_ENABLE_GPU_ASSISTED_EXT,
  VK_VALIDATION_FEATURE_ENABLE_GPU_ASSISTED_RESERVE_BINDING_SLOT_EXT,
  VK_VALIDATION_FEATURE_ENABLE_BEST_PRACTICES_EXT,
  VK_VALIDATION_FEATURE_ENABLE_DEBUG_PRINTF_EXT,
  VK_VALIDATION_FEATURE_ENABLE_SYNCHRONIZATION_VALIDATION_EXT,
};
static const u32 c_validation_feature_count = @arrlen(c_validation_features);

// ****************************************************************************
static const VkDebugUtilsMessageSeverityFlagsEXT c_callback_severities = 
    VK_DEBUG_UTILS_MESSAGE_SEVERITY_VERBOSE_BIT_EXT
  | VK_DEBUG_UTILS_MESSAGE_SEVERITY_INFO_BIT_EXT
  | VK_DEBUG_UTILS_MESSAGE_SEVERITY_WARNING_BIT_EXT
  | VK_DEBUG_UTILS_MESSAGE_SEVERITY_ERROR_BIT_EXT;

// ****************************************************************************
static const VkDebugUtilsMessageTypeFlagsEXT c_callback_types = 
    VK_DEBUG_UTILS_MESSAGE_TYPE_GENERAL_BIT_EXT
  | VK_DEBUG_UTILS_MESSAGE_TYPE_VALIDATION_BIT_EXT
  | VK_DEBUG_UTILS_MESSAGE_TYPE_PERFORMANCE_BIT_EXT;

// ****************************************************************************
static const char* c_enabled_extensions[] =
{
  VK_KHR_SURFACE_EXTENSION_NAME,
$ if IRO_LINUX then
  VK_KHR_XLIB_SURFACE_EXTENSION_NAME,
$ elseif IRO_WIN32 then --if IRO_LINUX
  VK_KHR_WIN32_SURFACE_EXTENSION_NAME,
$ else -- elseif IRO_WIN32
$ error "unhandled platform"
$ end -- if IRO_LINUX
$ if ECS_DEBUG then
  VK_EXT_DEBUG_UTILS_EXTENSION_NAME,
$ end -- if ECS_DEBUG
};
static const u32 c_enabled_extension_count = @arrlen(c_enabled_extensions);

// ****************************************************************************
static const char* c_required_device_extensions[] = 
{
  VK_KHR_SWAPCHAIN_EXTENSION_NAME,
};
static const u32 c_required_device_extension_count = 
  @arrlen(c_required_device_extensions);

// ****************************************************************************
static const VkDescriptorPoolSize c_pool_sizes[] =
{
  { VK_DESCRIPTOR_TYPE_SAMPLER,                1000 },
  { VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, 1000 },
  { VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE,          1000 },
  { VK_DESCRIPTOR_TYPE_STORAGE_IMAGE,          1000 },
  { VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER,   1000 },
  { VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER,   1000 },
  { VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,         1000 },
  { VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,         1000 },
  { VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC, 1000 },
  { VK_DESCRIPTOR_TYPE_STORAGE_BUFFER_DYNAMIC, 1000 },
  { VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT,       1000 }
};
static const u32 c_pool_size_count = @arrlen(c_pool_sizes);

// ****************************************************************************
// Constants determining the max amount of each graphics object we will 
// manage the gpu memory of internally.
enum 
{
  c_MaxImages = 512,
  c_MaxBuffers = 512,
  c_MaxShaders = 32,
  c_MaxPipelines = 8,
  c_MaxDefaultFramebuffers = 4,
};

// ****************************************************************************
// TODO(sushi) ideally these are eventually removed, and we allow defining 
//             these stuff through the api as well as through data.
static const u32 c_uniform_buffer_descriptor_idx = 0;
static const u32 c_sampler_descriptor_set_index = 1;
static const u32 c_storage_buffer_description_index = 2;

/* ============================================================================
 *  Objects used to manage device memory manually.
 */
struct DeviceHeapAllocation
{
  u32 memory_type;
  u32 block_index;
  VkDeviceSize offset;
  VkDeviceSize size;
  VkDeviceSize aligned_offset;
  VkDeviceSize aligned_size;
};

struct DeviceHeapBlock
{
  VkDeviceSize size;
  VkDeviceMemory memory;
  u32 cursor;
  u32 mapped_count;
  void* mapped_data;
};

struct DeviceHeap
{
  VkMemoryHeap heap;
  VkDeviceSize alignment;
  VkDeviceSize preferred_block_size;
  Array<DeviceHeapBlock> blocks;
  Array<DeviceHeapAllocation> allocations;
  Array<DeviceHeapAllocation> free_chunks;
};

/* ============================================================================
 */
struct DeviceBuffer
{
  VkBuffer handle;
  DeviceHeapAllocation* ptr;

  void reset()
  {
    handle = VK_NULL_HANDLE;
    ptr = nullptr;
  }

  b8 isValid() const
  {
    return handle != VK_NULL_HANDLE && ptr != nullptr;
  }
};

/* ============================================================================
 */
struct DeviceImage
{
  VkImage handle;
  DeviceHeapAllocation* ptr;

  void reset() 
  { 
    handle = VK_NULL_HANDLE; 
    ptr = nullptr;
  }

  b8 isValid() const 
  { 
    return handle != VK_NULL_HANDLE && ptr != nullptr;
  }
};

/* ============================================================================
 *  TODO(sushi) remove
 */
struct DeviceShader
{
  VkShaderModule module;
  VkShaderStageFlagBits stage;

  void reset()
  {
    module = VK_NULL_HANDLE;
    stage = {};
  }

  b8 isValid() const
  {
    return 
      module != VK_NULL_HANDLE;
  }
};

/* ============================================================================
 */
struct DevicePipeline
{
  VkPipeline pipeline;
  VkPipelineLayout layout;

  void reset()
  {
    pipeline = VK_NULL_HANDLE;
    layout = VK_NULL_HANDLE;
  }

  b8 isValid() const
  {
    return 
      pipeline != VK_NULL_HANDLE &&
      layout != VK_NULL_HANDLE;
  }
};

/* ============================================================================
 */
struct DeviceSampler
{
  VkSampler handle;

  b8 isValid() const 
  {
    return handle != VK_NULL_HANDLE;
  }
};

/* ============================================================================
 */
template<typename T, u64 capacity>
struct ResourcePool
{ 
  FixedPool<T, capacity> pool;
  u32 count;

  T* add()
  {
    T* elem = pool.add();
    if (elem == nullptr)
      return nullptr;
    count += 1;
    return elem;
  }

  void remove(T* elem)
  {
    assert(elem != nullptr);
    elem->reset();
    pool.remove(elem);
    count -= 1;
  }

  // We assert that any internally stored resource is valid, since 
  // they should never get corrupted for the time we store them.
  T* get(u32 index)
  {
    T* elem = pool.atIndex(index);
    if (elem == nullptr)
      return nullptr;
    assert(elem->isValid());
    return elem;
  }

  u32 indexOf(T* elem)
  {
    return pool.indexOf(elem);
  }
};

/* ============================================================================
 *  All state relevant to the Vulkan graphics backend as well as its 
 *  functionality.
 */
struct Vulkan
{
  //// Renderer Objects ////

  // TODO(sushi) maybe rework these to store a salt value.
  ResourcePool<DeviceBuffer,   c_MaxBuffers>   buffer_pool;
  ResourcePool<DeviceImage,    c_MaxImages>    image_pool;
  ResourcePool<DeviceShader,   c_MaxShaders>   shader_pool;
  ResourcePool<DevicePipeline, c_MaxPipelines> pipeline_pool;

  StackArray<DeviceHeap, VK_MAX_MEMORY_TYPES> heaps;

  //// Vulkan Core Objects ////

  VkAllocationCallbacks allocator_callbacks;

  VkInstance instance;

  VkDebugUtilsMessengerEXT debug_messenger;
  PFN_vkCmdBeginDebugUtilsLabelEXT func_vkCmdBeginDebugUtilsLabelEXT;
  PFN_vkCmdEndDebugUtilsLabelEXT func_vkCmdEndDebugUtilsLabelEXT;
  PFN_vkCmdInsertDebugUtilsLabelEXT func_vkCmdInsertDebugUtilsLabelEXT;
  PFN_vkSetDebugUtilsObjectNameEXT func_vkSetDebugUtilsObjectNameEXT;

  VkPhysicalDevice physical_device;
  VkPhysicalDeviceProperties physical_device_properties;
  VkPhysicalDeviceMemoryProperties physical_device_memory_properties;
  VkPhysicalDeviceFeatures physical_device_features;
  VkPhysicalDeviceFeatures physical_device_enabled_features;
  u32 physical_device_graphics_queue_family;
  u32 physical_device_present_queue_family;

  VkDevice device;
  VkQueue graphics_queue;
  VkQueue present_queue;

  VkCommandPool command_pool;
  VkCommandBuffer command_buffer;

  VkDescriptorPool descriptor_pool;

  VkSurfaceKHR surface;
  VkSurfaceFormatKHR surface_format;
  VkPresentModeKHR surface_present_mode;
  VkExtent2D surface_extent;

  VkSwapchainKHR swapchain;
  u32 swapchain_min_image_count;
  u32 swapchain_image_count;

  struct SwapchainBuffer
  {
    VkImage image;
    VkImageView view;
  };

  Array<SwapchainBuffer> swapchain_buffers;

  VkSemaphore image_acquired_semaphore;
  VkSemaphore render_complete_semaphore;

  VkPipelineCache pipeline_cache;

  struct InitParams
  {
    mem::Allocator* allocator;
    mem::LenientBump& temp_allocator;

    Window& window;
  };

  b8 init(const InitParams& params);

  // Updates the swapchain to the given viewport size.
  b8 updateSwapchain(vec2i viewport_size, mem::LenientBump& temp_allocator);

  static Vulkan* fromRendererInternal(RendererInternal* ri)
  {
    return (Vulkan*)(ri + 1);
  }

  // Initialization functions.
  
  b8 initInstance(
    mem::Allocator* allocator,
    mem::LenientBump& temp_allocator);
  b8 initSurface(Window& window);
  b8 initPhysicalDevice(
    mem::Allocator* allocator,
    mem::LenientBump& temp_allocator);
  b8 initLogicalDevice();
  b8 initCommandPool();
  b8 initCommandBuffer();
  b8 initDescriptorPools();
  b8 initSwapchain(vec2i viewport_size, mem::LenientBump& temp_allocator);
  b8 initSyncObjects();
  b8 initPipelineCache();

  // Internal graphics memory management.

  DeviceHeapAllocation* allocate(
    u32 memory_type,
    VkMemoryRequirements memory_requirements);

  void deallocate(DeviceHeapAllocation* ptr);

  DeviceHeap* getHeap(DeviceHeapAllocation* ptr)
  {
    return &heaps[ptr->memory_type];
  }

  DeviceHeapBlock* getHeapBlock(DeviceHeapAllocation* ptr)
  {
    return &getHeap(ptr)->blocks[ptr->block_index];
  }

  DeviceHeapBlock* getHeapBlock(DeviceHeap* heap, DeviceHeapAllocation* ptr)
  {
    return &heap->blocks[ptr->block_index];
  }

  VkDeviceMemory getDeviceMemory(DeviceHeapAllocation* ptr)
  {
    return getHeapBlock(ptr)->memory;
  }

  // Makes an internal allocation for the given buffer and binds it 
  // to the buffer on the device.
  DeviceHeapAllocation* allocateAndBindVkBufferMemory(
    VkBuffer buffer,
    VkMemoryPropertyFlags properties);
  DeviceHeapAllocation* allocateAndBindVkBufferMemory(
    VkBuffer buffer,
    VkMemoryPropertyFlags properties,
    VkMemoryRequirements memreq);

  // Makes an internal allocation for the given image and binds it 
  // to the buffer on the device.
  DeviceHeapAllocation* allocateAndBindVkImageMemory(VkImage image);
  DeviceHeapAllocation* allocateAndBindVkImageMemory(
    VkImage image,
    VkMemoryRequirements memreq);

  // Vulkan object creation and management helpers.

  // Does not make any internal allocation, use allocateVkBuffer to do so.
  b8 createVkBuffer(
    VkBuffer* out,
    VkDeviceSize size,
    VkBufferUsageFlags usage,
    String debug_name);

  void destroyVkBuffer(VkBuffer buffer)
    { vkDestroyBuffer(device, buffer, &allocator_callbacks); }

  b8 createVkImage(
    VkImage*          out,
    vec2u             size,
    VkFormat          format,
    VkImageUsageFlags usage,
    String            debug_name);

  void destroyVkImage(VkImage image)
    { vkDestroyImage(device, image, &allocator_callbacks); }

  b8 createVkImageView(
    VkImageView* out,
    VkImage image,
    VkFormat format,
    const VkComponentMapping& components,
    String debug_name);

  void destroyVkImageView(VkImageView view)
    { vkDestroyImageView(device, view, &allocator_callbacks); }

  b8 createVkSampler(
    VkSampler* out,
    VkFilter filter,
    VkSamplerAddressMode address_mode,
    String debug_name);

  void destroyVkSampler(VkSampler sampler)
    { vkDestroySampler(device, sampler, &allocator_callbacks); }

  b8 createVkDescriptorSetLayout(
    VkDescriptorSetLayout* out,
    Slice<VkDescriptorSetLayoutBinding> bindings,
    String debug_name);

  void destroyVkDescriptorSetLayout(VkDescriptorSetLayout layout)
    { vkDestroyDescriptorSetLayout(device, layout, &allocator_callbacks); }

  b8 allocateVkDescriptorSet(
    VkDescriptorSet* out,
    const VkDescriptorSetLayout* layout,
    String debug_name);

  void deallocateVkDescriptorSet(VkDescriptorSet set)
    { vkFreeDescriptorSets(device, descriptor_pool, 1, &set); }

  void updateVkDescriptorSet_Buffer(
    VkDescriptorSet set,
    VkDescriptorType type,
    u32 binding,
    u32 array_offset,
    Slice<VkDescriptorBufferInfo> buffers);

  void updateVkDescriptorSet_Image(
    VkDescriptorSet set,
    u32 array_offset,
    u32 binding,
    Slice<VkDescriptorImageInfo> images);

  b8 createVkShaderModule(
    VkShaderModule* out,
    Bytes spv_binary,
    String debug_name);
    
  void destroyVkShaderModule(VkShaderModule shader)
    { vkDestroyShaderModule(device, shader, &allocator_callbacks); }

  b8 createVkPipelineLayout(
    VkPipelineLayout* out,
    Slice<VkDescriptorSetLayout> set_layouts,
    Slice<VkPushConstantRange> push_constants,
    String debug_name);

  void destroyVkPipelineLayout(VkPipelineLayout layout)
    { vkDestroyPipelineLayout(device, layout, &allocator_callbacks); }

  b8 createVkPipeline(
    VkPipeline* out,
    VkPipelineLayout layout,
    VkShaderModule vert_shader,
    VkShaderModule frag_shader,
    b8 has_vertex_input,
    String debug_name);

  void destroyVkPipeline(VkPipeline pipeline)
    { vkDestroyPipeline(device, pipeline, &allocator_callbacks); }

  // Misc. helpers for doing common Vulkan things.

  // Begin/end a command buffer for temporary use, eg. copying pixels to 
  // a texture.
  b8 beginSingleUseCommandBuffer(VkCommandBuffer* out);
  b8 endSingleUseCommandBuffer(VkCommandBuffer command_buffer);

  b8 mapVkBuffer(void** out, VkDeviceSize size, DeviceHeapAllocation* ptr);
  void unmapVkBuffer(DeviceHeapAllocation* ptr);
  b8 flushMappedVkBuffer(
    VkDeviceSize offset,
    VkDeviceSize size,
    DeviceHeapAllocation* ptr);

  b8 mapCopyAndFlushVkBufferMemory(
    void* data, 
    VkDeviceSize size,
    DeviceHeapAllocation* ptr);

  // Create a buffer meant for temporary use, typically for queueing a 
  // write to a device-only gfx object.
  b8 createStagingBuffer(
    VkBuffer* out,
    DeviceHeapAllocation** out_allocation,
    VkDeviceSize required_size);

  b8 stageVkBufferMemory(
    void* data, 
    VkDeviceSize size, 
    VkBuffer buffer,
    VkDeviceSize buffer_required_size);

  b8 stageVkImageMemory(
    void* data, 
    VkDeviceSize size, 
    VkImage image,
    vec2u image_size);

  u32 determineMemoryType(
    VkMemoryRequirements memory_requirements,
    VkMemoryPropertyFlags memory_property_flags);

  // Command functions.

  void cmdImageMemoryBarrier(
    VkCommandBuffer command_buffer, 
    VkImage image,
    VkAccessFlags src_access,
    VkAccessFlags dst_access,
    VkImageLayout old_layout,
    VkImageLayout new_layout,
    VkPipelineStageFlags src_stage,
    VkPipelineStageFlags dst_stage);

  void cmdCopyBufferToImage(
    VkCommandBuffer command_buffer,
    VkImage image,
    VkBuffer buffer,
    vec2u size);

  void cmdBeginRendering(
    VkCommandBuffer command_buffer,
    VkRect2D render_area,
    VkImage image,
    VkImageView image_view,
    VkClearValue clear_value);

  void cmdEndRendering(VkCommandBuffer command_buffer);

  /* --------------------------------------------------------------------------
   *  Attaches a name to a Vulkan object that will appear in tools like 
   *  RenderDoc. Ideally, all of the objects we create are named.
   */
  void debugSetObjectName(
    VkObjectType type, 
    void* handle, 
    iro::io::Formattable auto... args)
  {
  $ if ECS_DEBUG then
    if (func_vkSetDebugUtilsObjectNameEXT == nullptr)
      return;
    if (handle == nullptr)
      return;

    iro::io::StaticBuffer<64> name = {};
    iro::io::formatv(&name, args...);

    VkDebugUtilsObjectNameInfoEXT name_info =
    {
      .sType = VK_STRUCTURE_TYPE_DEBUG_UTILS_OBJECT_NAME_INFO_EXT,
      .objectType = type,
      .objectHandle = (u64)handle,
      .pObjectName = (const char*)name.buffer,
    };

    func_vkSetDebugUtilsObjectNameEXT(device, &name_info);
  $ end -- if ECS_DEBUG
  }
  
  // Data tracking the current render state of the Vulkan instance.
  struct RenderState
  {
    // The command buffer of the current render pass instance, if we are in 
    // one. Eg. this is VK_NULL_HANDLE if vkCmdBeginRendering has not been
    // called yet.
    VkCommandBuffer command_buffer;

    // The currently bound pipeline.
    DevicePipeline* bound_pipeline;
  };

  RenderState state;
};

/* ----------------------------------------------------------------------------
 */
struct VulkanResultInfo
{
  String name;
  String desc;
  b8 is_error;
};

static VulkanResultInfo getVkResultInfo(VkResult result)
{
$$$
local strings = 
{
  VK_SUCCESS = "Command completed successfully",
  VK_NOT_READY = "A fence or query has not yet completed",
  VK_TIMEOUT = "A wait operation has not completed in the specified time",
  VK_EVENT_SET = "An event was signaled",
  VK_EVENT_RESET = "An event is unsignaled",
  VK_INCOMPLETE = "A return array was too small for the result",
  VK_SUBOPTIMAL_KHR = 
    "A swapchain no longer matches the surface properties exactly, but can "..
    "still be used to preset to the surface successfully",
  VK_THREAD_IDLE_KHR = 
    "A deferred operation is not complete but there is currently no work "..
    "for this thread to do at the time of this call.",
  VK_THREAD_DONE_KHR = 
    "A deferred operation is not complete but there is no work remaining "..
    "to assign to additional threads.",
-- NOTE(sushi) not defined for us rn.
-- VK_OPERATION_DEFERRED = 
--   "A deferred operation was requested and at least some of the work was "..
--   "deferred",
-- VK_OPERATION_NOT_DEFERRED_KHR = 
--   "A deferred operation was requested and no operations were deferred.",
  VK_PIPELINE_COMPILE_REQUIRED =
    "A requested pipeline creation would have required compilation, but "..
    "the application requested it not be performed.",
  VK_PIPELINE_BINARY_MISSING_KHR = 
    "The application attempted to create a pipeline binary by querying an "..
    "internal cache, but the internal cache entry did not exist.",
  VK_INCOMPATIBLE_SHADER_BINARY_EXT = 
    "The provided binary shader code is not compatible with this device.",
  VK_ERROR_OUT_OF_HOST_MEMORY = "A host memory allocation has failed",
  VK_ERROR_OUT_OF_DEVICE_MEMORY = "A device memory allocation has failed",
  VK_ERROR_INITIALIZATION_FAILED = 
    "Initialization of an object could not be completely for "..
    "implementation-specific reasons.",
  VK_ERROR_DEVICE_LOST = "The logical or physical device has been lost.",
  VK_ERROR_MEMORY_MAP_FAILED = "Mapping of a memory object has failed.",
  VK_ERROR_LAYER_NOT_PRESENT = 
    "A requested layer is not present or could not be loaded.",
  VK_ERROR_EXTENSION_NOT_PRESENT = "A requested extension is not supported.",
  VK_ERROR_FEATURE_NOT_PRESENT = "A requested feature is not supported.",
  VK_ERROR_INCOMPATIBLE_DRIVER = 
    "The requested version of Vulkan is not supported by the driver or is "..
    "otherwise incompatible for implementation-specific reasons.",
  VK_ERROR_TOO_MANY_OBJECTS = 
    "Too many objects of the type have already been created.",
  VK_ERROR_FORMAT_NOT_SUPPORTED = 
    "A requested format is not supported on this device.",
  VK_ERROR_FRAGMENTED_POOL = 
    "A pool allocation has failed due to fragmentation of the pool's "..
    "memory. This must only be returned if no attempt to allocate host or "..
    "device memory was made to accommodate the new allocation. This ".. 
    "should be returned in preference to VK_ERROR_OUT_OF_POOL_MEMORY, ".. 
    "but only if the implementation is certain that the pool allocation "..
    "failure was due to fragmentation.",
  VK_ERROR_SURFACE_LOST_KHR = "A surface is no longer available.",
  VK_ERROR_NATIVE_WINDOW_IN_USE_KHR = 
    "The requested window is already in use by Vulkan or another API in "..
    "a manner which prevents it from being used again.",
  VK_ERROR_OUT_OF_DATE_KHR = 
    "A surface has changed in such a way that it is no longer compatible "..
    "with the swapchain, and further presentation requests using the ".. 
    "swapchain will fail. Applications must query the new surface ".. 
    "properties and recreate their swapchain if they wish to continue "..
    "presenting to the surface.",
  VK_ERROR_INCOMPATIBLE_DISPLAY_KHR = 
    "The display used by a swapchain does not use the same presentable ".. 
    "image layout, or is incompatible in a way that prevents sharing an ".. 
    "image.",
  VK_ERROR_INVALID_SHADER_NV = 
    "One or more shaders failed to compile or link.",
  VK_ERROR_OUT_OF_POOL_MEMORY = 
    "A pool memory allocation has failed. This must only be returned if "..
    "no attempt to allocate host or device memory was made to ".. 
    "accommodate the new allocation. If the failure was definitely due "..
    "to fragmentation of the pool, VK_ERROR_FRAGMENTED_POOL should be "..
    "returned instead.",
  VK_ERROR_INVALID_EXTERNAL_HANDLE = 
    "An external handle is not a valid handle of the specified type.",
  VK_ERROR_FRAGMENTATION = 
    "A descriptor pool creation has failed due to fragmentation.",
  VK_ERROR_INVALID_DEVICE_ADDRESS_EXT = 
    "A buffer creation failed because the requested address is not available.",
-- NOTE(sushi) same as the above... handle somehow later.
-- VK_ERROR_INVALID_OPAQUE_CAPTURE_ADDRESS = 
--   "A buffer creation or memory allocation failed because the requested ".. 
--   "address is not available. A shader group handle assignment failed "..
--   "because the requested shader group handle information is no longer "..
--   "valid.",
  VK_ERROR_FULL_SCREEN_EXCLUSIVE_MODE_LOST_EXT = 
    "An operation on a swapchain created with ".. 
    "VK_FULL_SCREEN_EXCLUSIVE_APPLICATION_CONTROLLED_EXT failed as it ".. 
    "did not have exclusive full-screen access. This may occur due to "..
    "implementation-dependent reasons, outside of the applicationâ€™s control.",
  VK_ERROR_VALIDATION_FAILED_EXT = 
    "A command failed because invalid usage was detected by the "..
    "implementation or a validation-layer.",
  VK_ERROR_COMPRESSION_EXHAUSTED_EXT = 
    "An image creation failed because internal resources required for "..
    "compression are exhausted. This must only be returned when "..
    "fixed-rate compression is requested.",
  VK_ERROR_IMAGE_USAGE_NOT_SUPPORTED_KHR = 
    "The requested VkImageUsageFlags are not supported.",
  VK_ERROR_VIDEO_PICTURE_LAYOUT_NOT_SUPPORTED_KHR = 
    "The requested video picture layout is not supported.",
  VK_ERROR_VIDEO_PROFILE_OPERATION_NOT_SUPPORTED_KHR = 
    "A video profile operation specified via "..
    "VkVideoProfileInfoKHR::videoCodecOperation is not supported.",
  VK_ERROR_VIDEO_PROFILE_FORMAT_NOT_SUPPORTED_KHR = 
    "Format parameters in a requested VkVideoProfileInfoKHR chain are "..
    "not supported.",
  VK_ERROR_VIDEO_PROFILE_CODEC_NOT_SUPPORTED_KHR = 
    "Codec-specific parameters in a requested VkVideoProfileInfoKHR chain "..
    "are not supported.",
  VK_ERROR_VIDEO_STD_VERSION_NOT_SUPPORTED_KHR = 
    "The specified video Std header version is not supported.",
  VK_ERROR_INVALID_VIDEO_STD_PARAMETERS_KHR = 
    "The specified Video Std parameters do not adhere to the syntactic or "..
    "semantic requirements of the used video compression standard, or "..
    "values derived from parameters according to the rules defined by "..
    "the used video compression standard do not adhere to the "..
    "capabilities of the video compression standard or the implementation.",
  VK_ERROR_NOT_PERMITTED = 
    "The driver implementation has denied a request to acquire a priority "..
    "above the default priority (VK_QUEUE_GLOBAL_PRIORITY_MEDIUM_EXT) "..
    "because the application does not have sufficient privileges.",
  VK_ERROR_NOT_ENOUGH_SPACE_KHR = 
    "The application did not provide enough space to return all "..
    "the required data.",
  VK_ERROR_UNKNOWN = 
    "An unknown error has occurred; either the application has "..
    "provided invalid input, or an implementation failure has occurred. :(",
}
$$$

  VulkanResultInfo info = {};

  switch (result)
  {
$ for k,v in pairs(strings) do
  case $(k): 
    info.name = "$(k)"_str;
    info.desc = "$(v)"_str;
    info.is_error = $(k:find "^VK_ERROR" and "true" or "false");
    break;
$ end
  default:
    info.name = "<<UNKNOWN VkResult>>"_str;
    info.desc = "<<UNKNOWN VkResult>>"_str;
    info.is_error = true;
    break;
  }

  return info;
}

/* ----------------------------------------------------------------------------
 */
static VKAPI_ATTR VkBool32 VKAPI_CALL vkDebugCallback(
    VkDebugUtilsMessageSeverityFlagBitsEXT severity,
    VkDebugUtilsMessageTypeFlagsEXT type,
    const VkDebugUtilsMessengerCallbackDataEXT* data,
    void* user_data)
{
  Vulkan* vk = (Vulkan*)user_data;

  if (vk == nullptr || vk->command_pool == VK_NULL_HANDLE)
    return VK_FALSE;

  switch (severity)
  {
  case VK_DEBUG_UTILS_MESSAGE_SEVERITY_VERBOSE_BIT_EXT:
    TRACE(data->pMessage, '\n');
    break;
  case VK_DEBUG_UTILS_MESSAGE_SEVERITY_INFO_BIT_EXT:
    INFO(data->pMessage, '\n');
    break;
  case VK_DEBUG_UTILS_MESSAGE_SEVERITY_WARNING_BIT_EXT:
    WARN(data->pMessage, '\n');
  case VK_DEBUG_UTILS_MESSAGE_SEVERITY_ERROR_BIT_EXT:
    ERROR(data->pMessage, '\n');
    break;
  }

  return VK_FALSE;
}

/* ----------------------------------------------------------------------------
 */
template<typename... Args>
static b8 handleVkCallResult(
    VkResult result,
    String funcname,
    Args... args)
{
  VulkanResultInfo info = getVkResultInfo(result);

  if (result == VK_SUCCESS)
  {
$ if print_success then
    TRACE(funcname, " succeeded\n");
$ end
  }
  else
  {
    if (info.is_error)
    {
      ERROR(
        funcname, " returned ", info.name, ": ", info.desc, "\n",
        args..., "\n");

      return false;
    }
    else
    {
$ if print_non_error_success_as_warning then
      WARN(funcname, " returned ", info.name, ": ", info.desc, "\n");
$ end
    }
  }

  return true;
}

$ -- Helper that wraps calls to vulkan functions that return a VkResult.
$ -- Used to ensure that we print detailed information about what went 
$ -- wrong in a call rather than just saying that a call failed if we don't
$ -- get VK_SUCCESS back.
$ local function vkc(call, ...)
$   local funcname = call:match "^([%w%d_]+)"
$   local args = cmn.joinArgs(',', ...)
  if (!handleVkCallResult($(call), "$(funcname)"_str 
         $(args and ","..args or "")))
    return false;
$ end

$ local function vkcr(call, ret, ...)
$   local funcname = call:match "^([%w%d_]+)"
$   local args = cmn.joinArgs(',', ...)
  if (!handleVkCallResult($(call), "$(funcname)"_str 
         $(args and ","..args or "")))
    return $(ret);
$ end

$ local function vkcnr(call, ...)
$   local funcname = call:match "^([%w%d_]+)"
$   local args = cmn.joinArgs(',', ...)
  handleVkCallResult($(call), "$(funcname)"_str 
           $(args and ","..args or ""))
$ end


/* ----------------------------------------------------------------------------
 */
b8 Vulkan::init(const InitParams& params)
{
  if (!initInstance(params.allocator, params.temp_allocator))
    return false;

  if (!initSurface(params.window))
    return false;

  if (!initPhysicalDevice(params.allocator, params.temp_allocator))
    return false;

  if (!initLogicalDevice())
    return false;

  if (!initCommandPool())
    return false;

  if (!initCommandBuffer())
    return false;

  if (!initDescriptorPools())
    return false;

  if (!initSwapchain(params.window.size, params.temp_allocator))
    return false;

  if (!initSyncObjects())
    return false;

  if (!initPipelineCache())
    return false;

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::updateSwapchain(
    vec2i viewport_size,
    mem::LenientBump& temp_allocator)
{
  VkSwapchainKHR old_swapchain = swapchain;

  VkSurfaceCapabilitiesKHR capabilities;

  @vkc(vkGetPhysicalDeviceSurfaceCapabilitiesKHR(
    physical_device,
    surface, 
    &capabilities))

  u32 format_count = 0;
  
  @vkc(vkGetPhysicalDeviceSurfaceFormatsKHR(
    physical_device,
    surface, 
    &format_count, 
    nullptr))
  assert(format_count != 0 && "surface should not have been selected");

  auto formats =
    temp_allocator.allocateType<VkSurfaceFormatKHR>(format_count);
  if (formats == nullptr)
    return ERROR("failed to alloc memory for vulkan surface formats\n");

  @vkc(vkGetPhysicalDeviceSurfaceFormatsKHR(
    physical_device,
    surface, 
    &format_count, 
    formats))

  surface_format = formats[0];
  for (u32 i = 0; i < format_count; i++)
  {
$ if IRO_LINUX then
    if (   formats[i].format == VK_FORMAT_R8G8B8A8_UNORM
        && formats[i].colorSpace == VK_COLOR_SPACE_EXTENDED_SRGB_LINEAR_EXT)
    {
      surface_format = formats[i];
      break;
    }
$ else
    if (   formats[i].format == VK_FORMAT_B8G8R8A8_SRGB
        && formats[i].colorSpace == VK_COLOR_SPACE_EXTENDED_SRGB_LINEAR_EXT)
    {
      surface_format = formats[i];
      break;
    }
$ end
  }

  u32 present_mode_count = 0;
  @vkc(vkGetPhysicalDeviceSurfacePresentModesKHR(
    physical_device,
    surface, 
    &present_mode_count, 
    nullptr))
  assert(present_mode_count != 0 && "surface should not have been selected");

  auto present_modes =
    temp_allocator.allocateType<VkPresentModeKHR>(present_mode_count);
  if (present_modes == nullptr)
    return FATAL("failed to alloc memory for vulkan surface present modes\n");

  @vkc(vkGetPhysicalDeviceSurfacePresentModesKHR(
    physical_device,
    surface, 
    &present_mode_count, 
    present_modes))

  b8 immediate = false;
  b8 fifo_relaxed = false;
  b8 mailbox = false;
  for (u32 i = 0; i < present_mode_count; i++)
  {
    switch (present_modes[i])
    {
      case VK_PRESENT_MODE_IMMEDIATE_KHR:
        immediate = true;
      break;
      case VK_PRESENT_MODE_MAILBOX_KHR:
        mailbox = true;
        break;
      case VK_PRESENT_MODE_FIFO_RELAXED_KHR:
        fifo_relaxed = true;
        break;
    }
  }

  //TODO(delle) user settings for present mode selection
  if (mailbox)
  {
    swapchain_min_image_count = 3;
    surface_present_mode = VK_PRESENT_MODE_MAILBOX_KHR;
    @log.info("selected mailbox\n");
  }
  else if (immediate)
  {
    swapchain_min_image_count = 1;
    surface_present_mode = VK_PRESENT_MODE_IMMEDIATE_KHR;
    @log.info("selected immediate\n");
  }
  else if (fifo_relaxed)
  {
    swapchain_min_image_count = 2;
    surface_present_mode = VK_PRESENT_MODE_FIFO_RELAXED_KHR;
    @log.info("selected relaxed\n");
  }
  else
  {
    swapchain_min_image_count = 3;
    surface_present_mode = VK_PRESENT_MODE_FIFO_KHR;
    @log.info("selected fifo\n");
  }

  swapchain_min_image_count = 
    max(capabilities.minImageCount,
        min(capabilities.maxImageCount, 
            swapchain_min_image_count));

  if (capabilities.currentExtent.width != 0xFFFFFFFF)
  {
    surface_extent = capabilities.currentExtent;
  }
  else
  {
    //TODO(delle) user settings for resolution selection
    surface_extent.width = 
      max(capabilities.minImageExtent.width,
          min(capabilities.maxImageExtent.width,
              (u32)viewport_size.x));

    surface_extent.height = 
      max(capabilities.minImageExtent.height,
          min(capabilities.maxImageExtent.height,
              (u32)viewport_size.y));
  }

  u32 queue_family_indices[2] =
  {
    physical_device_graphics_queue_family,
    physical_device_present_queue_family,
  };

  VkSwapchainCreateInfoKHR create_info =
  {
    .sType = VK_STRUCTURE_TYPE_SWAPCHAIN_CREATE_INFO_KHR,
    .surface = surface,
    .minImageCount = swapchain_min_image_count,
    .imageFormat = surface_format.format,
    .imageColorSpace = surface_format.colorSpace,
    .imageExtent = surface_extent,
    .imageArrayLayers = 1,
    .imageUsage = VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT
      | VK_IMAGE_USAGE_TRANSFER_DST_BIT,
    .preTransform = capabilities.currentTransform,
    .compositeAlpha = VK_COMPOSITE_ALPHA_OPAQUE_BIT_KHR,
    .presentMode = surface_present_mode,
    .clipped = VK_TRUE,
    .oldSwapchain = old_swapchain,
  };

  if (   physical_device_graphics_queue_family
      != physical_device_present_queue_family)
  {
    create_info.imageSharingMode = VK_SHARING_MODE_CONCURRENT;
    create_info.queueFamilyIndexCount = 2;
    create_info.pQueueFamilyIndices = queue_family_indices;
  }
  else
  {
    create_info.imageSharingMode = VK_SHARING_MODE_EXCLUSIVE;
    create_info.queueFamilyIndexCount = 0;
    create_info.pQueueFamilyIndices = nullptr;
  }

  @vkc(vkCreateSwapchainKHR(
    device,
    &create_info,
    &allocator_callbacks,
    &swapchain))

  vkDestroySwapchainKHR(device, old_swapchain, &allocator_callbacks);

  @vkc(vkGetSwapchainImagesKHR(
    device,
    swapchain,
    &swapchain_image_count,
    nullptr))

  swapchain_buffers.resize(swapchain_image_count);

  auto swapchain_images = Array<VkImage>::create(swapchain_image_count);
  defer { swapchain_images.destroy(); };

  @vkc(vkGetSwapchainImagesKHR(
    device,
    swapchain,
    &swapchain_image_count,
    swapchain_images.arr));
  
  io::StaticBuffer<128> image_view_name;
  for (s32 i = 0; i < swapchain_image_count; ++i)
  {
    image_view_name.clear();
    io::formatv(&image_view_name, "swapchain ", i);

    swapchain_buffers[i].image = swapchain_images[i];

    if (!createVkImageView(
          &swapchain_buffers[i].view,
          swapchain_buffers[i].image,
          surface_format.format,
          {},
          image_view_name.asStr()))
      return ERROR("failed to create swapchain ", i, " image view\n");
  }
    
  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initInstance(
    mem::Allocator* allocator,
    mem::LenientBump& temp_allocator)
{
  DEBUG("creating vulkan allocator\n");

  auto alloc_func = [](void* user_data, size_t size,
    size_t alignment, VkSystemAllocationScope scope) -> void*
  {
    auto* allocator = (mem::Allocator*)user_data;
    size_t aligned_size = (size+alignment-1) & ~(alignment-1);
    void* result = allocator->allocate(aligned_size);
    if (result == nullptr)
    {
      ERROR("failed to alloc memory for vulkan\n");
      return nullptr;
    }
    assert((size_t)result % alignment == 0); //invalid alignment
    return result;
  };

  auto realloc_func = [](void* user_data, void* ptr, size_t size,
    size_t alignment, VkSystemAllocationScope scope) -> void*
  {
    auto* allocator = (mem::Allocator*)user_data;
    size_t aligned_size = (size+alignment-1) & ~(alignment-1);
    void* result = allocator->reallocate(ptr, aligned_size);
    if (result == nullptr)
    {
      ERROR("failed to realloc memory for vulkan\n");
      return nullptr;
    }
    assert((size_t)result % alignment == 0); //invalid alignment
    return result;
  };

  auto free_func = [](void* user_data, void* ptr)
  {
    if (ptr == nullptr)
      return;
    auto* allocator = (mem::Allocator*)user_data;
    allocator->free(ptr);
  };

  allocator_callbacks.pUserData = allocator;
  allocator_callbacks.pfnAllocation = alloc_func;
  allocator_callbacks.pfnReallocation = realloc_func;
  allocator_callbacks.pfnFree = free_func;

  //TODO(delle) internal allocation tracking
  //ri.allocator.pfnInternalAllocation = ;
  //ri.allocator.pfnInternalFree = ;

  DEBUG("creating vulkan instance\n");

$ if ECS_DEBUG then

  u32 layer_count = 0;
  @vkc(vkEnumerateInstanceLayerProperties(
      &layer_count, 
      nullptr),
    "failed to get instance layer count")

  auto layer_properties =
    temp_allocator.allocateType<VkLayerProperties>(layer_count);
  if (layer_properties == nullptr)
    return FATAL("failed to alloc memory for vulkan layer properties\n");

  @vkc(vkEnumerateInstanceLayerProperties(
      &layer_count, 
      layer_properties),
    "failed to enumerate all vulkan instance layer properties")

  for (u32 i = 0; i < c_validation_layer_count; i++)
  {
    b8 layer_found = false;
    for (u32 j = 0; j < layer_count; j++)
    {
      if (!strcmp(c_validation_layers[i], layer_properties[j].layerName))
      {
        layer_found = true;
        break;
      }
    }

    if (!layer_found)
      return FATAL(
        "a required vulkan validation layer '", c_validation_layers[i], 
        "' was requested but not available on the device\n");
  }

  VkValidationFeaturesEXT validation_features =
  {
    .sType = VK_STRUCTURE_TYPE_VALIDATION_FEATURES_EXT,
    .enabledValidationFeatureCount = c_validation_feature_count,
    .pEnabledValidationFeatures = c_validation_features,
    .disabledValidationFeatureCount = 0,
    .pDisabledValidationFeatures = nullptr,
  };

  VkDebugUtilsMessengerCreateInfoEXT debug_create_info =
  {
    .sType = VK_STRUCTURE_TYPE_DEBUG_UTILS_MESSENGER_CREATE_INFO_EXT,
    .pNext = &validation_features,
    .messageSeverity = c_callback_severities,
    .messageType = c_callback_types,
    .pfnUserCallback = vkDebugCallback,
    .pUserData = this,
  };
$ end -- if ECS_DEBUG

  VkApplicationInfo app_info =
  {
    .sType = VK_STRUCTURE_TYPE_APPLICATION_INFO,
    .pApplicationName = c_app_name,
    .applicationVersion = c_app_version,
    .pEngineName = c_engine_name,
    .engineVersion = c_engine_version,
    .apiVersion = c_vulkan_api_version,
  };

  VkInstanceCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
$ if ECS_DEBUG then
    .pNext = &debug_create_info,
$ end -- if ECS_DEBUG
    .pApplicationInfo = &app_info,
$ if ECS_DEBUG then
    .enabledLayerCount = c_validation_layer_count,
    .ppEnabledLayerNames = c_validation_layers,
$ end -- if ECS_DEBUG
    .enabledExtensionCount = c_enabled_extension_count,
    .ppEnabledExtensionNames = c_enabled_extensions,
  };

  @vkc(vkCreateInstance(&create_info, &allocator_callbacks, &instance))

$ if ECS_DEBUG then
  {
    DEBUG("creating vulkan debug messenger and utils\n");

    VkDebugUtilsMessengerCreateInfoEXT create_info =
    {
      .sType = VK_STRUCTURE_TYPE_DEBUG_UTILS_MESSENGER_CREATE_INFO_EXT,
      .messageSeverity = c_callback_severities,
      .messageType = c_callback_types,
      .pfnUserCallback = vkDebugCallback,
      .pUserData = this,
    };

    auto vkCreateDebugUtilsMessengerEXT = 
      (PFN_vkCreateDebugUtilsMessengerEXT)vkGetInstanceProcAddr(
        instance, 
        "vkCreateDebugUtilsMessengerEXT");

    if (vkCreateDebugUtilsMessengerEXT == nullptr)
      return FATAL(
        "failed to retrieve proc vkCreateDebugUtilsMessengerEXT\n");

    @vkc(vkCreateDebugUtilsMessengerEXT(
      instance, &create_info, &allocator_callbacks, &debug_messenger))

    func_vkCmdBeginDebugUtilsLabelEXT =
      (PFN_vkCmdBeginDebugUtilsLabelEXT)vkGetInstanceProcAddr(
        instance, 
        "vkCmdBeginDebugUtilsLabelEXT");

    func_vkCmdEndDebugUtilsLabelEXT =
      (PFN_vkCmdEndDebugUtilsLabelEXT)vkGetInstanceProcAddr(
        instance, 
        "vkCmdEndDebugUtilsLabelEXT");

    func_vkCmdInsertDebugUtilsLabelEXT =
      (PFN_vkCmdInsertDebugUtilsLabelEXT)vkGetInstanceProcAddr(
        instance, 
        "vkCmdInsertDebugUtilsLabelEXT");

    func_vkSetDebugUtilsObjectNameEXT =
      (PFN_vkSetDebugUtilsObjectNameEXT)vkGetInstanceProcAddr(
        instance, 
        "vkSetDebugUtilsObjectNameEXT");
  }
$ end

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initSurface(Window& window)
{
  DEBUG("creating vulkan surface\n");

$ if IRO_LINUX then
  VkXlibSurfaceCreateInfoKHR create_info =
  {
    .sType = VK_STRUCTURE_TYPE_XLIB_SURFACE_CREATE_INFO_KHR,
    .dpy = x11.display,
    .window = (X11Window)window.handle,
  };

  @vkc(vkCreateXlibSurfaceKHR(
    instance, &create_info, &allocator_callbacks, &surface))

$ elseif IRO_WIN32 then --if IRO_LINUX

  VkWin32SurfaceCreateInfoKHR create_info =
  {
    .sType = VK_STRUCTURE_TYPE_WIN32_SURFACE_CREATE_INFO_KHR,
    .hinstance = win32.h_console_instance,
    .hwnd = (HWND)window.handle,
  };

  @vkc(vkCreateWin32SurfaceKHR(
    instance, &create_info, &allocator_callbacks, &surface))

$ else -- elseif IRO_WIN32
$ error "unhandled platform"
$ end -- if IRO_LINUX

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initPhysicalDevice(
    mem::Allocator* allocator,
    mem::LenientBump& temp_allocator)
{
  DEBUG("creating the vulkan physical device\n");

  u32 device_count = 0;

  @vkc(vkEnumeratePhysicalDevices(instance, &device_count, nullptr))

  auto devices =
    temp_allocator.allocateType<VkPhysicalDevice>(device_count);
  if (devices == nullptr)
    return FATAL("failed to alloc memory for vulkan physical devices\n");

  @vkc(vkEnumeratePhysicalDevices(instance, &device_count, devices))

  for (u32 i = 0; i < device_count; i++)
  {
    u32 family_count = 0;
    vkGetPhysicalDeviceQueueFamilyProperties(
      devices[i], &family_count, nullptr);

    auto families =
      temp_allocator.allocateType<VkQueueFamilyProperties>(family_count);
    if (families == nullptr)
      return FATAL("failed to alloc memory for vulkan queue families\n");

    vkGetPhysicalDeviceQueueFamilyProperties(
      devices[i], &family_count, families);

    b8 found_graphics_family = false;
    b8 found_present_family = false;
    for (u32 j = 0; j < family_count; j++)
    {
      if (   (families[j].queueFlags & VK_QUEUE_GRAPHICS_BIT)
          && (families[j].queueFlags & VK_QUEUE_COMPUTE_BIT))
      {
        physical_device_graphics_queue_family = j;
        found_graphics_family = true;
      }

      VkBool32 present_support = false;
      @vkc(vkGetPhysicalDeviceSurfaceSupportKHR(
        devices[i], j, surface, &present_support))

      if (present_support == VK_TRUE)
      {
        physical_device_present_queue_family = j;
        found_present_family = true;
      }

      if (found_graphics_family && found_present_family)
        break;
    }
    if (!found_graphics_family || !found_present_family)
      continue; //next physical device

    u32 device_extension_count;
    @vkc(vkEnumerateDeviceExtensionProperties(
      devices[i], nullptr, &device_extension_count, nullptr))

    auto device_extensions =
      temp_allocator.allocateType<VkExtensionProperties>(
        device_extension_count);
    if (device_extensions == nullptr)
      return FATAL("failed to alloc memory for vulkan physical device"
        " extensions\n");

    @vkc(vkEnumerateDeviceExtensionProperties(
      devices[i], nullptr, &device_extension_count, device_extensions))

    u32 extensions_supported = 0;
    for (u32 j = 0; j < device_extension_count; j++)
    {
      for (u32 k = 0; k < c_required_device_extension_count; k++)
      {
        if (!strcmp(device_extensions[j].extensionName,
                    c_required_device_extensions[k]))
        {
          extensions_supported++;
          break;
        }
      }
      if (extensions_supported == c_required_device_extension_count)
        break;
    }
    if (extensions_supported != c_required_device_extension_count)
      continue; //next physical device

    u32 format_count = 0;
    u32 present_mode_count = 0;
    
    @vkc(vkGetPhysicalDeviceSurfaceFormatsKHR(
      devices[i], surface, &format_count, nullptr))

    @vkc(vkGetPhysicalDeviceSurfacePresentModesKHR(
      devices[i], surface, &format_count, nullptr))

    if (format_count == 0 || present_mode_count == 0)
      continue; //next physical device

    physical_device = devices[i];
    break; //found suitable physical device
  }

  if (physical_device == VK_NULL_HANDLE)
    return FATAL("failed to find a suitable vulkan physical device\n");

  vkGetPhysicalDeviceProperties(
    physical_device, &physical_device_properties);

  vkGetPhysicalDeviceMemoryProperties(
    physical_device, &physical_device_memory_properties);

  vkGetPhysicalDeviceFeatures(
    physical_device, &physical_device_features);

  auto& mem_props = physical_device_memory_properties;
  heaps.len = mem_props.memoryTypeCount;
  for (u32 i = 0; i < mem_props.memoryTypeCount; i++)
  {
    heaps[i].heap =
      mem_props.memoryHeaps[mem_props.memoryTypes[i].heapIndex];

    heaps[i].alignment =
      max(physical_device_properties.limits.nonCoherentAtomSize,
          (VkDeviceSize)32);

    if (heaps[i].heap.size <= (VkDeviceSize)unit::megabytes(1024))
    {
      heaps[i].preferred_block_size =
        alignUp(heaps[i].heap.size / 8, heaps[i].alignment);
    }
    else
    {
      heaps[i].preferred_block_size = unit::megabytes(256);
    }
    heaps[i].blocks.init(8, allocator);
    heaps[i].allocations.init(512, allocator);
    heaps[i].free_chunks.init(8, allocator);
  }

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initLogicalDevice()
{
  TRACE("creating vulkan logical device\n");

  f32 queue_priority = 1.0f;
  u32 queue_create_info_count = 1;
  VkDeviceQueueCreateInfo queue_create_infos[2] = {};
  queue_create_infos[0].sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
  queue_create_infos[0].queueFamilyIndex =
    physical_device_graphics_queue_family;
  queue_create_infos[0].queueCount = 1;
  queue_create_infos[0].pQueuePriorities = &queue_priority;

  if (   physical_device_graphics_queue_family
      != physical_device_present_queue_family)
  {
    queue_create_info_count = 2;
    queue_create_infos[1].sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
    queue_create_infos[1].queueFamilyIndex =
      physical_device_present_queue_family;
    queue_create_infos[1].queueCount = 1;
    queue_create_infos[1].pQueuePriorities = &queue_priority;
  }

  if (physical_device_features.samplerAnisotropy)
  {
    physical_device_enabled_features.samplerAnisotropy = VK_TRUE;
    physical_device_enabled_features.sampleRateShading = VK_TRUE;
  }

  if (physical_device_features.fillModeNonSolid)
  {
    physical_device_enabled_features.fillModeNonSolid = VK_TRUE;
    if (physical_device_features.wideLines)
      physical_device_enabled_features.wideLines = VK_TRUE;
  }

  if (physical_device_features.geometryShader)
    physical_device_enabled_features.geometryShader = VK_TRUE;

  VkDeviceCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
    .queueCreateInfoCount = queue_create_info_count,
    .pQueueCreateInfos = queue_create_infos,
    .enabledExtensionCount = c_required_device_extension_count,
    .ppEnabledExtensionNames = c_required_device_extensions,
    .pEnabledFeatures = &physical_device_enabled_features,
  };

  @vkc(vkCreateDevice(
    physical_device,
    &create_info,
    &allocator_callbacks,
    &device))

  vkGetDeviceQueue(
    device,
    physical_device_graphics_queue_family,
    0, 
    &graphics_queue);

  vkGetDeviceQueue(
    device,
    physical_device_present_queue_family,
    0, 
    &present_queue);

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initCommandPool()
{
  DEBUG("creating vulkan command pool\n");

  VkCommandPoolCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO,
    .flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT,
    .queueFamilyIndex =
      physical_device_graphics_queue_family,
  };

  @vkc(vkCreateCommandPool(
    device, &create_info, &allocator_callbacks, &command_pool))

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initCommandBuffer()
{
  DEBUG("creating the vulkan command buffer\n");
  VkCommandBufferAllocateInfo alloc_info =
  {
    .sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
    .commandPool = command_pool,
    .level = VK_COMMAND_BUFFER_LEVEL_PRIMARY,
    .commandBufferCount = 1,
  };

  @vkc(vkAllocateCommandBuffers(device, &alloc_info, &command_buffer))

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initDescriptorPools()
{
  DEBUG("creating vulkan descriptor pools\n");
  VkDescriptorPoolCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO,
    .flags = VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT,
    .maxSets = 1000 * c_pool_size_count,
    .poolSizeCount = c_pool_size_count,
    .pPoolSizes = c_pool_sizes,
  };

  @vkc(vkCreateDescriptorPool(
    device, &create_info, &allocator_callbacks, &descriptor_pool))

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initSwapchain(
    vec2i viewport_size,
    mem::LenientBump& temp_allocator)
{
  DEBUG("creating vulkan swapchain\n");

  if (!updateSwapchain(viewport_size, temp_allocator))
    return false;

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initSyncObjects()
{
  DEBUG("creating vulkan sync objects\n");

  VkSemaphoreCreateInfo semaphore_info =
  {
    .sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO,
  };

  @vkc(vkCreateSemaphore(
    device,
    &semaphore_info,
    &allocator_callbacks,
    &image_acquired_semaphore))

  debugSetObjectName(
    VK_OBJECT_TYPE_SEMAPHORE,
    image_acquired_semaphore, 
    "<image acquired semaphore>");

  @vkc(vkCreateSemaphore(
    device,
    &semaphore_info,
    &allocator_callbacks,
    &render_complete_semaphore))

  debugSetObjectName(
    VK_OBJECT_TYPE_SEMAPHORE,
    render_complete_semaphore, 
    "<render complete semaphore>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::initPipelineCache()
{
  DEBUG("creating the vulkan pipeline cache\n");

  using namespace fs;
  if (File::exists("data/"_str) && File::exists("data/pipelines.cache"_str))
  {
    auto file = File::from("data/pipelines.cache"_str, OpenFlag::Read);
    if (notnil(file))
    {
      defer { file.close(); };

      io::Memory buffer;
      buffer.open();
      defer { buffer.close(); };

      u64 filesize = file.getInfo().byte_size;
      if (filesize == buffer.consume(&file, filesize))
      {
        VkPipelineCacheCreateInfo create_info =
        {
          .sType = VK_STRUCTURE_TYPE_PIPELINE_CACHE_CREATE_INFO,
          .initialDataSize = (size_t)filesize,
          .pInitialData = buffer.ptr,
        };

        @vkc(vkCreatePipelineCache(
          device,
          &create_info,
          &allocator_callbacks,
          &pipeline_cache))
      }
    }
  }

  return true;
}

/* ----------------------------------------------------------------------------
 */
u32 Vulkan::determineMemoryType(
    VkMemoryRequirements memory_requirements,
    VkMemoryPropertyFlags memory_property_flags)
{
  auto& mem_props = physical_device_memory_properties;

  for (u32 i = 0; i < mem_props.memoryTypeCount; i++)
  {
    if (!(memory_requirements.memoryTypeBits & (1 << i)))
      continue;

    if ((mem_props.memoryTypes[i].propertyFlags & memory_property_flags)
        != memory_property_flags)
      continue;

    return i;
  }

  return 0;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkImage(
    VkImage*          out,
    vec2u             size,
    VkFormat          format,
    VkImageUsageFlags usage,
    String            debug_name)
{
  VkImageCreateInfo create_info =
  {
    .sType         = VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO,
    .imageType     = VK_IMAGE_TYPE_2D,
    .format        = format,
    .mipLevels     = 1,
    .arrayLayers   = 1,
    .samples       = VK_SAMPLE_COUNT_1_BIT,
    .tiling        = VK_IMAGE_TILING_OPTIMAL,
    .usage         = usage,
    .sharingMode   = VK_SHARING_MODE_EXCLUSIVE,
    .initialLayout = VK_IMAGE_LAYOUT_UNDEFINED,
    .extent =
    {
      .width = size.x,
      .height = size.y,
      .depth = 1
    },
  };

  @vkc(vkCreateImage(
      device,
      &create_info,
      &allocator_callbacks,
      out),
    "failed to create vulkan image ", debug_name)

  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_IMAGE,
      *out,
      '<', debug_name, " image>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkImageView(
    VkImageView* out,
    VkImage image,
    VkFormat format,
    const VkComponentMapping& components,
    String debug_name)
{
  VkImageViewCreateInfo create_info =
  {
    .sType      = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO,
    .image      = image,
    .viewType   = VK_IMAGE_VIEW_TYPE_2D,
    .format     = format,
    .components = components,
    .subresourceRange =
    {
      .aspectMask = VK_IMAGE_ASPECT_COLOR_BIT,
      .levelCount = 1,
      .layerCount = 1,
    }
  };

  @vkc(vkCreateImageView(
      device,
      &create_info,
      &allocator_callbacks,
      out),
    "failed to create vulkan image view ", debug_name)

  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_IMAGE_VIEW,
      *out,
      '<', debug_name, " image>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkBuffer(
    VkBuffer* out,
    VkDeviceSize size,
    VkBufferUsageFlags usage,
    String debug_name)
{
  VkBufferCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO,
    .size = size,
    .usage = usage,
    .sharingMode = VK_SHARING_MODE_EXCLUSIVE,
  };

  @vkc(vkCreateBuffer(
      device,
      &create_info,
      &allocator_callbacks,
      out),
    "failed to create VkBuffer ", debug_name)

  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_BUFFER,
      *out,
      '<', debug_name, " buffer>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
DeviceHeapAllocation* Vulkan::allocateAndBindVkBufferMemory(
    VkBuffer buffer,
    VkMemoryPropertyFlags properties)
{
  VkMemoryRequirements memreq;
  vkGetBufferMemoryRequirements(device, buffer, &memreq);
  
  return allocateAndBindVkBufferMemory(buffer, properties, memreq);
}

/* ----------------------------------------------------------------------------
 */
DeviceHeapAllocation* Vulkan::allocateAndBindVkBufferMemory(
    VkBuffer buffer,
    VkMemoryPropertyFlags properties,
    VkMemoryRequirements memreq)
{
  u32 memtype = determineMemoryType(memreq, properties);

  DeviceHeapAllocation* ptr = allocate(memtype, memreq);
  if (ptr == nullptr)
  {
    ERROR("failed to allocate memory for VkBuffer\n");
    return nullptr;
  }

  @vkcr(vkBindBufferMemory(
      device,
      buffer,
      getDeviceMemory(ptr),
      ptr->aligned_offset), nullptr,
    "failed to bind memory of VkBuffer\n")

  return ptr;
}

/* ----------------------------------------------------------------------------
 */
DeviceHeapAllocation* Vulkan::allocateAndBindVkImageMemory(VkImage image)
{
  VkMemoryRequirements memreq;
  vkGetImageMemoryRequirements(device, image, &memreq);
  
  return allocateAndBindVkImageMemory(image, memreq);
}

/* ----------------------------------------------------------------------------
 */
DeviceHeapAllocation* Vulkan::allocateAndBindVkImageMemory(
    VkImage image,
    VkMemoryRequirements memreq)
{
  u32 memtype = determineMemoryType(
    memreq, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT);

  DeviceHeapAllocation* ptr = allocate(memtype, memreq);
  if (ptr == nullptr)
  {
    ERROR("failed to allocate memory for VkImage\n");
    return nullptr;
  }

  @vkcr(vkBindImageMemory(
      device,
      image,
      getDeviceMemory(ptr),
      ptr->aligned_offset), nullptr,
    "failed to bind memory of VkImage\n")

  return ptr;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::allocateVkDescriptorSet(
    VkDescriptorSet* out,
    const VkDescriptorSetLayout* layout,
    String debug_name)
{
  VkDescriptorSetAllocateInfo alloc_info =
  {
    .sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO,
    .descriptorPool = descriptor_pool,
    .descriptorSetCount = 1,
    .pSetLayouts = layout,
  };

  @vkc(vkAllocateDescriptorSets(
    device,
    &alloc_info,
    out))

  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_BUFFER,
      *out,
      '<', debug_name, " descriptor>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::updateVkDescriptorSet_Buffer(
    VkDescriptorSet set,
    VkDescriptorType type,
    u32 binding,
    u32 array_offset,
    Slice<VkDescriptorBufferInfo> buffers)
{
  VkWriteDescriptorSet descriptor_set_write =
  {
    .sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
    .dstSet = set,
    .dstBinding = binding,
    .dstArrayElement = array_offset,
    .descriptorCount = u32(buffers.len),
    .descriptorType = type,
    .pBufferInfo = buffers.ptr,
  };

  vkUpdateDescriptorSets(device, 1, &descriptor_set_write, 0, nullptr);
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::updateVkDescriptorSet_Image(
    VkDescriptorSet set,
    u32 array_offset,
    u32 binding,
    Slice<VkDescriptorImageInfo> images)
{
  VkWriteDescriptorSet descriptor_set_write =
  {
    .sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
    .dstSet = set,
    .dstBinding = binding,
    .dstArrayElement = array_offset,
    .descriptorCount = u32(images.len),
    .descriptorType = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
    .pImageInfo = images.ptr,
  };

  vkUpdateDescriptorSets(device, 1, &descriptor_set_write, 0, nullptr);
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkSampler(
    VkSampler* out,
    VkFilter filter,
    VkSamplerAddressMode address_mode,
    String debug_name)
{
  VkSamplerCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO,
    .magFilter = filter,
    .minFilter = filter,
    .mipmapMode = VK_SAMPLER_MIPMAP_MODE_LINEAR,
    .addressModeU = address_mode,
    .addressModeV = address_mode,
    .addressModeW = address_mode,
    .mipLodBias = 0.0f,
    .anisotropyEnable = VK_FALSE,
    .maxAnisotropy = 1.0f,
    .compareEnable = VK_FALSE,
    .minLod = 0.0f,
    .maxLod = 0.0f,
    .borderColor = VK_BORDER_COLOR_FLOAT_OPAQUE_WHITE,
    .unnormalizedCoordinates = VK_FALSE,
  };

  VkSampler sampler;
  @vkc(vkCreateSampler(
      device,
      &create_info,
      &allocator_callbacks,
      out),
    "failed to create VkSampler '", debug_name, "'\n")

  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_SAMPLER,
      *out,
      '<', debug_name, " sampler>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkDescriptorSetLayout(
    VkDescriptorSetLayout* out,
    Slice<VkDescriptorSetLayoutBinding> bindings,
    String debug_name)
{
  VkDescriptorSetLayoutCreateInfo create_info = 
  {
    .sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO,
    .bindingCount = u32(bindings.len),
    .pBindings = bindings.ptr,
  };

  VkDescriptorSetLayout layout;
  @vkc(vkCreateDescriptorSetLayout(
      device, 
      &create_info,
      &allocator_callbacks, 
      &layout),
    "failed to create VkDescriptorSetLayout '", debug_name, "'\n")
  
  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_DESCRIPTOR_SET_LAYOUT,
      *out,
      '<', debug_name, " descriptor set layout>");
  
  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkShaderModule(
    VkShaderModule* out,
    Bytes spv_binary,
    String debug_name)
{
  VkShaderModuleCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
    .codeSize = spv_binary.len,
    .pCode = (const u32*)spv_binary.ptr,
  };

  @vkc(vkCreateShaderModule(
      device,
      &create_info,
      &allocator_callbacks,
      out),
    "failed to create VkShaderModule '", debug_name, "'\n")

  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_SHADER_MODULE,
      *out,
      '<', debug_name, " shader>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkPipelineLayout(
    VkPipelineLayout* out,
    Slice<VkDescriptorSetLayout> set_layouts,
    Slice<VkPushConstantRange> push_constants,
    String debug_name)
{
  VkPipelineLayoutCreateInfo create_info =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
    .setLayoutCount = u32(set_layouts.len),
    .pSetLayouts = set_layouts.ptr,
    .pushConstantRangeCount = u32(push_constants.len),
    .pPushConstantRanges = push_constants.ptr,
  };

  @vkc(vkCreatePipelineLayout(
      device,
      &create_info,
      &allocator_callbacks,
      out),
    "failed to create VkPipelineLayout '", debug_name, "'\n")
  
  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_PIPELINE_LAYOUT,
      *out,
      '<', debug_name, " pipeline layout>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createVkPipeline(
    VkPipeline* out,
    VkPipelineLayout layout,
    VkShaderModule vert_shader,
    VkShaderModule frag_shader,
    b8 has_vertex_input,
    String debug_name)
{

  VkPipelineVertexInputStateCreateInfo vertex_input_info =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_VERTEX_INPUT_STATE_CREATE_INFO,
  };

  if (has_vertex_input)
  {
    VkVertexInputBindingDescription vertex_binding =
    {
      .binding = 0,
      .stride = sizeof(Vertex),
      .inputRate = VK_VERTEX_INPUT_RATE_VERTEX,
    };

    VkVertexInputAttributeDescription vertex_attributes[] =
    {
      {
        .location = 0,
        .binding = 0,
        .format = VK_FORMAT_R32G32_SFLOAT,
        .offset = offsetof(Vertex, pos),
      },
      {
        .location = 1,
        .binding = 0,
        .format = VK_FORMAT_R32G32_SFLOAT,
        .offset = offsetof(Vertex, uv),
      },
      {
        .location = 2,
        .binding = 0,
        .format = VK_FORMAT_R8G8B8A8_UNORM,
        .offset = offsetof(Vertex, color),
      },
    };
    constexpr u32 vertex_attribute_count =
      sizeof(vertex_attributes) / sizeof(vertex_attributes[0]);

    vertex_input_info.vertexBindingDescriptionCount = 1;
    vertex_input_info.pVertexBindingDescriptions = &vertex_binding;
    vertex_input_info.vertexAttributeDescriptionCount = vertex_attribute_count;
    vertex_input_info.pVertexAttributeDescriptions = vertex_attributes;
  }

  VkPipelineInputAssemblyStateCreateInfo input_assembly =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_INPUT_ASSEMBLY_STATE_CREATE_INFO,
    .topology = VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST,
    .primitiveRestartEnable = VK_FALSE,
  };

  VkPipelineViewportStateCreateInfo viewport_state =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_VIEWPORT_STATE_CREATE_INFO,
    .viewportCount = 1,
    .scissorCount = 1,
  };

  VkPipelineRasterizationStateCreateInfo rasterizer =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_RASTERIZATION_STATE_CREATE_INFO,
    .depthClampEnable = VK_FALSE,
    .rasterizerDiscardEnable = VK_FALSE,
    .polygonMode = VK_POLYGON_MODE_FILL,
    .cullMode = VK_CULL_MODE_NONE,
    .frontFace = VK_FRONT_FACE_CLOCKWISE,
    .depthBiasEnable = VK_FALSE,
    .lineWidth = 1.0f,
  };

  VkPipelineMultisampleStateCreateInfo multisampling =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_MULTISAMPLE_STATE_CREATE_INFO,
    .rasterizationSamples = VK_SAMPLE_COUNT_1_BIT,
    .sampleShadingEnable = VK_FALSE,
  };

  VkPipelineColorBlendAttachmentState blend_attachment =
  {
    .blendEnable = VK_TRUE,
    .srcColorBlendFactor = VK_BLEND_FACTOR_SRC_ALPHA,
    .dstColorBlendFactor = VK_BLEND_FACTOR_ONE_MINUS_SRC_ALPHA,
    .colorBlendOp = VK_BLEND_OP_ADD,
    .srcAlphaBlendFactor = VK_BLEND_FACTOR_ONE,
    .dstAlphaBlendFactor = VK_BLEND_FACTOR_ONE_MINUS_SRC_ALPHA,
    .alphaBlendOp = VK_BLEND_OP_ADD,
    .colorWriteMask = 
        VK_COLOR_COMPONENT_R_BIT 
      | VK_COLOR_COMPONENT_G_BIT
      | VK_COLOR_COMPONENT_B_BIT 
      | VK_COLOR_COMPONENT_A_BIT,
  };

  VkPipelineColorBlendStateCreateInfo blend_state =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_COLOR_BLEND_STATE_CREATE_INFO,
    .logicOpEnable = VK_FALSE,
    .attachmentCount = 1,
    .pAttachments = &blend_attachment,
  };

  VkDynamicState dynamic_states[] =
  {
    VK_DYNAMIC_STATE_VIEWPORT,
    VK_DYNAMIC_STATE_SCISSOR
  };
  constexpr u32 dynamic_state_count =
    sizeof(dynamic_states) / sizeof(dynamic_states[0]);

  VkPipelineDynamicStateCreateInfo dynamic_state =
  {
    .sType = VK_STRUCTURE_TYPE_PIPELINE_DYNAMIC_STATE_CREATE_INFO,
    .dynamicStateCount = dynamic_state_count,
    .pDynamicStates = dynamic_states,
  };

  VkPipelineShaderStageCreateInfo shader_stages[2] =
  {
    {
      .sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
      .stage = VK_SHADER_STAGE_VERTEX_BIT,
      .module = vert_shader,
      .pName = "main",
    },
    {
      .sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
      .stage = VK_SHADER_STAGE_FRAGMENT_BIT,
      .module = frag_shader,
      .pName = "main",
    }
  };

  VkPipelineRenderingCreateInfoKHR pipeline_rendering_info = 
  {
    .colorAttachmentCount = 1,
    .pColorAttachmentFormats = &surface_format.format,
  };

  VkGraphicsPipelineCreateInfo pipeline_create_info =
  {
    .sType = VK_STRUCTURE_TYPE_GRAPHICS_PIPELINE_CREATE_INFO,
    .pNext = &pipeline_rendering_info,
    .stageCount = 2,
    .pStages = shader_stages,
    .pVertexInputState = &vertex_input_info,
    .pInputAssemblyState = &input_assembly,
    .pViewportState = &viewport_state,
    .pRasterizationState = &rasterizer,
    .pMultisampleState = &multisampling,
    .pColorBlendState = &blend_state,
    .pDynamicState = &dynamic_state,
    .layout = layout,
    .subpass = 0,
  };

  @vkc(vkCreateGraphicsPipelines(
      device,
      pipeline_cache,
      1,
      &pipeline_create_info,
      &allocator_callbacks,
      out),
    "failed to create pipeline '", debug_name, "'")

  if (notnil(debug_name))
    debugSetObjectName(
      VK_OBJECT_TYPE_PIPELINE,
      *out,
      '<', debug_name, " pipeline>");

  return true;
}

/* ----------------------------------------------------------------------------
 */
DeviceHeapAllocation* Vulkan::allocate(
  u32 memory_type,
  VkMemoryRequirements memory_requirements)
{
  DeviceHeap& heap = heaps[memory_type];

  VkDeviceSize alignment = max(memory_requirements.alignment, heap.alignment);
  VkDeviceSize aligned_size = alignUp(memory_requirements.size, alignment);

  if (aligned_size < heap.preferred_block_size)
  {
    for (DeviceHeapAllocation& free_chunk : heap.free_chunks)
    {
      VkDeviceSize chunk_aligned_offset = alignUp(free_chunk.offset, alignment);
      VkDeviceSize chunk_aligned_end = chunk_aligned_offset + aligned_size;
      if (chunk_aligned_end > free_chunk.offset + free_chunk.size)
        continue;

      VkDeviceSize size_after_alloc = free_chunk.size - aligned_size;
      if (size_after_alloc < alignment)
      {
        DeviceHeapBlock& block = heap.blocks[free_chunk.block_index];
        heap.allocations.push(free_chunk);
        heap.free_chunks.remove(&free_chunk);

        free_chunk.aligned_offset = chunk_aligned_offset;
        free_chunk.aligned_size = aligned_size;

        assert(isAligned(free_chunk.aligned_offset, alignment));
        assert(isAligned(free_chunk.aligned_size, alignment));
        return &free_chunk;
      }
      else
      {
        DeviceHeapBlock& block = heap.blocks[free_chunk.block_index];

        DeviceHeapAllocation* allocation = heap.allocations.push();
        allocation->memory_type = free_chunk.memory_type;
        allocation->block_index = free_chunk.block_index;
        allocation->offset = free_chunk.offset;
        allocation->size = aligned_size;
        allocation->aligned_offset = chunk_aligned_offset;
        allocation->aligned_size = aligned_size;

        free_chunk.size = size_after_alloc;
        free_chunk.offset = chunk_aligned_end;

        assert(isAligned(allocation->aligned_offset, alignment));
        assert(isAligned(allocation->aligned_size, alignment));
        return allocation;
      }
    }
  }

  DeviceHeapBlock* last_block = heap.blocks.last();
  if (heap.blocks.isEmpty() || 
      ((VkDeviceSize)last_block->cursor + aligned_size > last_block->size))
  {
    DeviceHeapBlock* block = heap.blocks.push();
    block->size = max(heap.preferred_block_size, aligned_size);
    block->cursor = 0;
    block->mapped_count = 0;
    block->mapped_data = nullptr;

    VkMemoryAllocateInfo alloc_info =
    {
      .sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO,
      .allocationSize = block->size,
      .memoryTypeIndex = memory_type,
    };

    if (!@vkcnr(vkAllocateMemory(
          device,
          &alloc_info,
          &allocator_callbacks,
          &block->memory),
          "failed to allocate memory for a vulkan heap block"))
    {
      heap.blocks.pop();
      return nullptr;
    }
  }

  u32 block_index = heap.blocks.len() - 1;
  DeviceHeapBlock& block = heap.blocks[block_index];

  DeviceHeapAllocation* allocation = heap.allocations.push();
  allocation->memory_type = memory_type;
  allocation->block_index = block_index;
  allocation->offset = (VkDeviceSize)block.cursor;
  allocation->size = aligned_size;
  allocation->aligned_offset = alignUp(allocation->offset, alignment);
  allocation->aligned_size = aligned_size;

  VkDeviceSize new_cursor = block.cursor + aligned_size;
  assert(new_cursor <= (VkDeviceSize)UINT32_MAX);
  block.cursor = (u32)new_cursor;

  assert(isAligned(allocation->aligned_offset, alignment));
  assert(isAligned(allocation->aligned_size, alignment));
  return allocation;
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::deallocate(DeviceHeapAllocation* ptr)
{
  if (ptr == nullptr)
    return;

  DeviceHeap& heap = heaps[ptr->memory_type];
  DeviceHeapBlock& block = heap.blocks[ptr->block_index];

  if (ptr->offset + ptr->size == block.cursor)
  {
    block.cursor -= ptr->size;
  }
  else
  {
    // see if we can merge with an existing free chunk
    DeviceHeapAllocation* merged_chunk = nullptr;
    for (DeviceHeapAllocation& chunk : heap.free_chunks)
    {
      if (chunk.block_index != ptr->block_index)
        continue;

      if (chunk.offset + chunk.size == ptr->offset)
      {
        chunk.size += ptr->size;
        merged_chunk = &chunk;
        break;
      }
      else if (ptr->offset + ptr->size == chunk.offset)
      {
        chunk.offset = ptr->offset;
        chunk.size += ptr->size;
        merged_chunk = &chunk;
        break;
      }
    }

    if (merged_chunk != nullptr)
    {
      // see if we can merge with a free chunk on the other side
      for (DeviceHeapAllocation& chunk : heap.free_chunks)
      {
        if (chunk.block_index != ptr->block_index)
          continue;
        if (&chunk == merged_chunk)
          continue;

        if (merged_chunk->offset + merged_chunk->size == chunk.offset)
        {
          merged_chunk->size += chunk.size;
          heap.free_chunks.remove(&chunk);
          break;
        }
        else if (chunk.offset + chunk.size == merged_chunk->offset)
        {
          merged_chunk->offset = chunk.offset;
          merged_chunk->size += chunk.size;
          heap.free_chunks.remove(&chunk);
          break;
        }
      }
    }
    else
    {
      heap.free_chunks.push(*ptr);
    }

    heap.allocations.remove(ptr);
  }
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::beginSingleUseCommandBuffer(VkCommandBuffer* out)
{
  VkCommandBufferAllocateInfo alloc_info =
  {
    .sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
    .level = VK_COMMAND_BUFFER_LEVEL_PRIMARY,
    .commandPool = command_pool,
    .commandBufferCount = 1,
  };

  @vkc(vkAllocateCommandBuffers(device, &alloc_info, out),
    "failed to allocate single use command buffer")

  VkCommandBufferBeginInfo begin_info =
  {
    .sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
    .flags = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
  };

  if (!@vkcnr(vkBeginCommandBuffer(*out, &begin_info),
          "failed to begin a single use command buffer"))
  {
    vkFreeCommandBuffers(device, command_pool, 1, out);
    return false;
  }

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::endSingleUseCommandBuffer(VkCommandBuffer command_buffer)
{
  @vkcnr(vkEndCommandBuffer(command_buffer),
    "failed to end single use command buffer");

  VkFenceCreateInfo fence_create_info =
  {
    .sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO,
  };

  VkFence fence = VK_NULL_HANDLE;
  @vkcnr(vkCreateFence(
      device, 
      &fence_create_info,
      &allocator_callbacks,
      &fence),
    "failed to create a fence when ending single use command buffer");

  VkSubmitInfo submit_info =
  {
    .sType = VK_STRUCTURE_TYPE_SUBMIT_INFO,
    .commandBufferCount = 1,
    .pCommandBuffers = &command_buffer,
  };

  @vkcnr(vkQueueSubmit(
      graphics_queue,
      1,
      &submit_info,
      fence),
    "failed to submit single use command buffer to the graphics queue");

  if (fence != VK_NULL_HANDLE)
  {
    ERROR("waiting for fence\n");

    @vkc(vkWaitForFences(
        device,
        1,
        &fence,
        VK_TRUE,
        TimeSpan::fromSeconds(100).toNanoseconds()),
      "failed to wait for a fence when ending single use command buffer");

    vkDestroyFence(device, fence, &allocator_callbacks);
  }
  else
  {
    vkQueueWaitIdle(graphics_queue);
  }

  vkFreeCommandBuffers(device, command_pool, 1, &command_buffer);

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::mapVkBuffer(
    void** out,
    VkDeviceSize size,
    DeviceHeapAllocation* ptr)
{
  DeviceHeapBlock* block = getHeapBlock(ptr);
  if (block->mapped_data == nullptr)
  {
    @vkc(vkMapMemory(
        device, 
        getDeviceMemory(ptr),
        ptr->aligned_offset,
        size,
        0,
        &block->mapped_data))

    block->mapped_count += 1;
  }

  if (out) 
    *out = block->mapped_data;

  return true;
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::unmapVkBuffer(DeviceHeapAllocation* ptr)
{
  DeviceHeapBlock* block = getHeapBlock(ptr);
  vkUnmapMemory(device, block->memory);
  block->mapped_data = nullptr;
  block->mapped_count -= 1;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::flushMappedVkBuffer(
    VkDeviceSize offset,
    VkDeviceSize size,
    DeviceHeapAllocation* ptr)
{
  DeviceHeap* heap = getHeap(ptr);
  DeviceHeapBlock* block = getHeapBlock(heap, ptr);

  if (size != VK_WHOLE_SIZE)
    size = alignUp(size, heap->alignment);

  VkMappedMemoryRange range =
  {
    .sType = VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE,
    .memory = block->memory,
    .offset = alignDown(ptr->aligned_offset + offset, heap->alignment),
    .size = size,
  };

  @vkc(vkFlushMappedMemoryRanges(
      device,
      1,
      &range));

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::mapCopyAndFlushVkBufferMemory(
    void* data,
    VkDeviceSize size,
    DeviceHeapAllocation* ptr)
{
  void* mapped;
  if (!mapVkBuffer(
        &mapped,
        size,
        ptr))
    return false;
  defer { unmapVkBuffer(ptr); };

  mem::copy(mapped, data, size);

  if (!flushMappedVkBuffer(0, VK_WHOLE_SIZE, ptr))
    return false;

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::createStagingBuffer(
    VkBuffer* out,
    DeviceHeapAllocation** out_allocation,
    VkDeviceSize required_size)
{
  if (!createVkBuffer(
        out,
        required_size,
        VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
        "staging buffer"_str))
    return ERROR("failed to create staging buffer\n");

  auto failsafe_destroy_buffer = deferWithCancel
  {
    destroyVkBuffer(*out);
  };

  DeviceHeapAllocation* staging_ptr = 
    allocateAndBindVkBufferMemory(
      *out, 
        VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT
      | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT);

  if (staging_ptr == nullptr)
    return ERROR("failed to allocate or bind staging buffer memory\n");

  failsafe_destroy_buffer.cancel();

  *out_allocation = staging_ptr;
  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::stageVkBufferMemory(
    void* data,
    VkDeviceSize size,
    VkBuffer buffer,
    VkDeviceSize buffer_required_size)
{
  VkBuffer staging_buffer;
  DeviceHeapAllocation* staging_ptr;
  if (!createStagingBuffer(
        &staging_buffer,
        &staging_ptr,
        buffer_required_size))
    return false;

  defer 
  { 
    destroyVkBuffer(staging_buffer);
    deallocate(staging_ptr);
  };

  void* staging_data = nullptr;
  if (!mapVkBuffer(
        &staging_data,
        buffer_required_size,
        staging_ptr))
    return ERROR("failed to map staging buffer\n");

  mem::copy(staging_data, data, size);

  unmapVkBuffer(staging_ptr);

  VkCommandBuffer command_buffer;
  if (!beginSingleUseCommandBuffer(&command_buffer))
    return ERROR("failed to begin command buffer for staging buffer\n");

  VkBufferCopy copy_region = 
  {
    .size = size,
  };

  vkCmdCopyBuffer(command_buffer, staging_buffer, buffer, 1, &copy_region);

  if (!endSingleUseCommandBuffer(command_buffer))
    return ERROR("failed to end command buffer for staging buffer\n");

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Vulkan::stageVkImageMemory(
    void* data, 
    VkDeviceSize size, 
    VkImage image,
    vec2u image_size)
{
  VkBuffer staging_buffer;
  DeviceHeapAllocation* staging_ptr;
  if (!createStagingBuffer(
        &staging_buffer,
        &staging_ptr,
        size))
    return false;

  defer 
  { 
    destroyVkBuffer(staging_buffer);
    deallocate(staging_ptr);
  };

  void* staging_data = nullptr;
  if (!mapVkBuffer(
        &staging_data,
        size,
        staging_ptr))
    return ERROR("failed to map staging buffer\n");

  mem::copy(staging_data, data, size);

  unmapVkBuffer(staging_ptr);

  VkCommandBuffer command_buffer;
  if (!beginSingleUseCommandBuffer(&command_buffer))
    return ERROR("failed to begin command buffer for staging buffer\n");

  cmdImageMemoryBarrier(
    command_buffer,
    image,
    // Accesses nothing.
    VK_ACCESS_NONE,
    // Accessed by transfer writes.
    VK_ACCESS_TRANSFER_WRITE_BIT,
    // Image starts in an undefined (cpu) layout.
    VK_IMAGE_LAYOUT_UNDEFINED,
    // Image should be transfered to a layout optimal for transfer.
    VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
    // Nothing to wait for.
    VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT,
    // Transfer stage commands need to wait on this barrier.
    VK_PIPELINE_STAGE_TRANSFER_BIT);

  cmdCopyBufferToImage(
    command_buffer,
    image,
    staging_buffer,
    image_size);

  cmdImageMemoryBarrier(
    command_buffer,
    image,
    // Accesses transfer writes.
    VK_ACCESS_TRANSFER_WRITE_BIT,
    // Accessed by shader reads.
    VK_ACCESS_SHADER_READ_BIT,
    // Image will be in optimal transfer layout.
    VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
    // Image should be converted to a layout optimal for shader reads.
    VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
    // Wait for transfer stage.
    VK_PIPELINE_STAGE_TRANSFER_BIT,
    // Fragment shader reads must wait on this barrier.
    VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT);

  if (!endSingleUseCommandBuffer(command_buffer))
    return ERROR("failed to end command buffer for staging buffer\n");
 
  return true;
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::cmdImageMemoryBarrier(
    VkCommandBuffer command_buffer, 
    VkImage image,
    VkAccessFlags src_access,
    VkAccessFlags dst_access,
    VkImageLayout old_layout,
    VkImageLayout new_layout,
    VkPipelineStageFlags src_stage,
    VkPipelineStageFlags dst_stage)
{
  VkImageMemoryBarrier barrier =
  {
    .sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER,
    .srcAccessMask = src_access,
    .dstAccessMask = dst_access,
    .oldLayout = old_layout,
    .newLayout = new_layout,
    .srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
    .dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED,
    .image = image,
    .subresourceRange =
    {
      .aspectMask = VK_IMAGE_ASPECT_COLOR_BIT,
      .levelCount = 1,
      .layerCount = 1,
    },
  };

  vkCmdPipelineBarrier(
    command_buffer,
    src_stage, 
    dst_stage,
    0, 
    0, 
    nullptr, 
    0, 
    nullptr, 
    1, 
    &barrier);
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::cmdCopyBufferToImage(
    VkCommandBuffer command_buffer,
    VkImage image,
    VkBuffer buffer,
    vec2u size)
{
  VkBufferImageCopy copy_region =
  {
    .imageSubresource =
    {
      .aspectMask = VK_IMAGE_ASPECT_COLOR_BIT,
      .layerCount = 1,
    },
    .imageOffset = {0, 0, 0},
    .imageExtent = {size.x, size.y, 1},
  };

  vkCmdCopyBufferToImage(
    command_buffer, 
    buffer, 
    image,
    VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 
    1, 
    &copy_region);
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::cmdBeginRendering(
    VkCommandBuffer command_buffer,
    VkRect2D render_area,
    VkImage image,
    VkImageView image_view,
    VkClearValue clear_value)
{
  // Transition image to the color attachment layout.
  cmdImageMemoryBarrier(
    command_buffer,
    image,
    VK_ACCESS_NONE,
    VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT,
    VK_IMAGE_LAYOUT_UNDEFINED,
    VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL,
    VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT,
    VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT);

  VkRenderingAttachmentInfo attachment_info = 
  {
    .sType = VK_STRUCTURE_TYPE_RENDERING_ATTACHMENT_INFO,
    .imageView = image_view,
    .imageLayout = VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL,
    .loadOp = VK_ATTACHMENT_LOAD_OP_CLEAR,
    .storeOp = VK_ATTACHMENT_STORE_OP_STORE,
    .clearValue = clear_value,
  };

  VkRenderingInfo render_info = 
  {
    .sType = VK_STRUCTURE_TYPE_RENDERING_INFO,
    .renderArea = render_area,
    .layerCount = 1,
    .colorAttachmentCount = 1,
    .pColorAttachments = &attachment_info,
  };

  vkCmdBeginRendering(command_buffer, &render_info);
}

/* ----------------------------------------------------------------------------
 */
void Vulkan::cmdEndRendering(VkCommandBuffer command_buffer)
{
  vkCmdEndRendering(command_buffer);
}

$ local function getvk()
  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);
$ end

$ local function defcerr(name)
$   return function(...)
  ERROR("while creating $(name) '", name, "': ", 
        $(cmn.joinArgs(',',...)), "\n");
$   end
$ end

/* ----------------------------------------------------------------------------
 */
Buffer Buffer::create(Renderer& renderer, const CreateParams& params)
{
  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  String name = resolved(params.debug_name, "unnamed"_str);

  DEBUG("creating a gfx::Buffer of size ", params.size, " named ", name, '\n');

  if (params.size == 0)
  {
    ERROR("cannot create gfx::Buffer of size 0\n");
    return nil;
  }

  if ((params.properties & MemoryProperty::LazilyAllocated) && 
      (params.properties & MemoryProperty::HostCoherent
                         | MemoryProperty::HostVisible))
  {
    ERROR("memory property LazilyAllocated is incompatible with HostVisible"
          "and HostCoherent\n");
    return nil;
  }

  if (params.behavior == MappingBehavior::Never &&
      (params.properties & MemoryProperty::HostVisible
                         | MemoryProperty::HostCoherent
                         | MemoryProperty::HostCached))
  {
    ERROR("mapping behavior Never is incompatible with memory properties"
          "HostVisible, HostCoherent, and HostCached\n");
    return nil;
  }

  VkBufferUsageFlags usage = 0;
  switch (params.usage)
  {
  case Usage::UniformBuffer: usage = VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT; break;
  case Usage::StorageBuffer: usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT; break;
  case Usage::IndexBuffer: usage = VK_BUFFER_USAGE_INDEX_BUFFER_BIT; break;
  case Usage::VertexBuffer: usage = VK_BUFFER_USAGE_VERTEX_BUFFER_BIT; break;
  }

  if (params.behavior == MappingBehavior::Never)
    usage |= VK_BUFFER_USAGE_TRANSFER_DST_BIT;

  VkBuffer handle;
  if (!vk->createVkBuffer(&handle, params.size, usage, name))
    return nil;

  auto failsafe_destroy_buffer = deferWithCancel
  {
    vk->destroyVkBuffer(handle);
  };

  VkMemoryPropertyFlags properties = 0;
$ local function mapprop(x, y)
  if (params.properties & MemoryProperty::$(x))
    properties |= VK_MEMORY_PROPERTY_$(y)_BIT;
$ end
  @mapprop(DeviceLocal,     DEVICE_LOCAL);
  @mapprop(HostVisible,     HOST_VISIBLE);
  @mapprop(HostCoherent,    HOST_COHERENT);
  @mapprop(HostCached,      HOST_CACHED);
  @mapprop(LazilyAllocated, LAZILY_ALLOCATED);

  VkMemoryRequirements memreq;
  vkGetBufferMemoryRequirements(vk->device, handle, &memreq);

  DeviceHeapAllocation* ptr = 
    vk->allocateAndBindVkBufferMemory(handle, properties, memreq);

  if (ptr == nullptr)
  {
    ERROR("failed to allocate and bind memory for VkBuffer ", name, "\n");
    return nil;
  }

  auto failsafe_deallocate = deferWithCancel
  {
    vk->deallocate(ptr);
  };

  switch (params.behavior)
  {
  case MappingBehavior::Never:
    if (params.data == nullptr)
    {
      WARN(
        "request to allocate a gfx::Buffer with mapping behavior Never. This "
        "type of request is mainly useful for intermediate compute shader "
        "memory, but compute shaders are not set up yet.\n");
    }
    else
    {
      if (!vk->stageVkBufferMemory(
            params.data,
            params.size,
            handle,
            memreq.size))
      {
        ERROR("failed to stage memory for gfx::Buffer '", name, "'\n");
        return nil;
      }
    }
    break;

  case MappingBehavior::Occasional:
    if (params.data != nullptr)
    {
      if (!vk->mapCopyAndFlushVkBufferMemory(
            params.data,
            params.size,
            ptr))
      {
        ERROR("failed to map/copy/flush initial data for gfx::Buffer '", 
              name, "\n'");
        return nil;
      }
    }
    break;

  case MappingBehavior::Persistent:
    {
      if (!vk->mapVkBuffer(nullptr, params.size, ptr))
      {
        ERROR("failed to map persistent gfx::Buffer '", name, "'\n");
        return nil;
      }

      if (params.data != nullptr)
      {
        mem::copy(
          vk->getHeapBlock(ptr)->mapped_data, params.data, params.size);
        vk->flushMappedVkBuffer(0, VK_WHOLE_SIZE, ptr);
      }
    }
    break;
  }

  DeviceBuffer* internal_buffer = vk->buffer_pool.add();
  internal_buffer->handle = handle;
  internal_buffer->ptr = ptr;

  Buffer result = {};
  result.index = vk->buffer_pool.indexOf(internal_buffer);
  result.mapped_data = vk->getHeapBlock(ptr)->mapped_data;

  failsafe_destroy_buffer.cancel();
  failsafe_deallocate.cancel();

  return result;
}

/* ----------------------------------------------------------------------------
 */
void Buffer::destroy(Renderer& renderer)
{
  if (isnil(*this))
    return;

  DEBUG("destroying gfx::Buffer at index ", index, "\n");

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  DeviceBuffer* internal_buffer = vk->buffer_pool.get(index);
  if (internal_buffer->handle == VK_NULL_HANDLE)
  {
    ERROR("attempt to destroy a gfx::Buffer at index ", index, " but its "
          "intenal backend handle is null\n");
    // Reset the index since this buffer is already invalid.
    *this = nil;
    // Also remove the invalid buffer from the pool since it shouldn't be in 
    // there.
    if (internal_buffer->ptr)
      vk->deallocate(internal_buffer->ptr);
    vk->buffer_pool.remove(internal_buffer);
    return;
  }

  if (mapped_data != nullptr)
    vk->unmapVkBuffer(internal_buffer->ptr);

  vk->destroyVkBuffer(internal_buffer->handle);
  vk->deallocate(internal_buffer->ptr);

  mapped_data = nullptr;

  index = 0;
  vk->buffer_pool.remove(internal_buffer);
}

/* ----------------------------------------------------------------------------
 */
b8 Buffer::map(Renderer& renderer)
{
  if (isnil(*this))
    return ERROR("attempt to map a nil gfx::Buffer\n");

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  // NOTE(sushi) we perform the follow checks before earlying out 
  //             on mapped_data != nullptr to ensure that the backend data
  //             is still valid.

  DeviceBuffer* internal_buffer = vk->buffer_pool.get(index);

  if (mapped_data != nullptr)
    return true;

  TRACE("mapping ", *this, "\n");

  void* mapped = nullptr;
  if (!vk->mapVkBuffer(&mapped, VK_WHOLE_SIZE, internal_buffer->ptr))
    return false;

  mapped_data = (u8*)mapped + internal_buffer->ptr->aligned_offset;
  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Buffer::flush(Renderer& renderer, u64 offset, u64 size)
{
  if (isnil(*this))
    return ERROR("attempt to flush a nil gfx::Buffer\n");

  if (size == FLUSH_WHOLE_BUFFER)
    TRACE("flushing whole buffer of ", *this, "\n");
  else
    TRACE("flushing ", size, " bytes of " , *this, " at offset ", offset, 
          "\n");

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  if (mapped_data == nullptr)
    return ERROR("attempt to flush unmapped ", *this, '\n');

  DeviceBuffer* internal_buffer = vk->buffer_pool.get(index);
  if (internal_buffer == nullptr)
  {
    ERROR("attempt to flush ", *this, " but its index is lost\n");
    *this = nil;
    return false;
  }

  VkDeviceSize dev_size = size;
  VkDeviceSize dev_offset = offset;

  if (size == FLUSH_WHOLE_BUFFER)
    size = VK_WHOLE_SIZE;

  // NOTE(sushi) size and offset are aligned inside this call.
  return vk->flushMappedVkBuffer(dev_offset, dev_size, internal_buffer->ptr);
}

/* ----------------------------------------------------------------------------
 */
b8 Buffer::copyAndFlush(Renderer& renderer, void* data, u64 size)
{
  if (isnil(*this))
    return ERROR("attempt to copy and flush a nil gfx::Buffer\n");

  TRACE("copying and flushing ", *this, "\n");

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  DeviceBuffer* internal_buffer = vk->buffer_pool.get(index);
  if (internal_buffer == nullptr)
    return ERROR("attempt to copy and flush ", *this, " but its index is lost"
                 "\n");

  if (mapped_data == nullptr)
    return ERROR("attempt to flush unmapped ", *this, "\n");

  mem::copy(mapped_data, data, size);

  VkDeviceSize dev_size = size;
  VkDeviceSize dev_offset = internal_buffer->ptr->aligned_offset;

  if (size == FLUSH_WHOLE_BUFFER)
    size = VK_WHOLE_SIZE;

  // NOTE(sushi) size and offset are aligned inside this call.
  return vk->flushMappedVkBuffer(dev_offset, dev_size, internal_buffer->ptr);
}

/* ----------------------------------------------------------------------------
 */
void Buffer::unmap(Renderer& renderer)
{
  if (isnil(*this))
  {
    ERROR("attempt to unmap a nil gfx::Buffer\n");
    return;
  }

  if (mapped_data == nullptr)
    return;

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  DeviceBuffer* internal_buffer = vk->buffer_pool.get(index);

  vk->unmapVkBuffer(internal_buffer->ptr);
  vk->deallocate(internal_buffer->ptr);
  mapped_data = nullptr;
}

/* ----------------------------------------------------------------------------
 */
static VkFormat imageFormatToVulkan(ImageFormat x)
{
$ local function map(x, y)
  case ImageFormat::$(x): return VK_FORMAT_$(y);
$ end
  switch (x)
  {
  @map(BW,   R8_UNORM)
  @map(BWA,  R8G8_UNORM)
  @map(RGB,  R8G8B8_UNORM)
  @map(RGBA, R8G8B8A8_UNORM)
  @map(BGRA, B8G8R8A8_UNORM)
  }

  ERROR("invalid ImageFormat ", (u32)x, "\n");
  return VK_FORMAT_UNDEFINED;
}

/* ----------------------------------------------------------------------------
 */
static u32 imageFormatToBytesPerPixel(ImageFormat x)
{
$ local function map(x, y)
  case ImageFormat::$(x): return $(y);
$ end
  switch (x)
  {
  @map(BW,   1)
  @map(BWA,  2)
  @map(RGB,  3)
  @map(RGBA, 4)
  @map(BGRA, 4)
  }

  ERROR("invalid ImageFormat ", (u32)x, "\n");
  return 0;
}

/* ----------------------------------------------------------------------------
 */
static VkFilter imageFilterToVulkan(ImageFilter x)
{
  switch (x)
  {
  case ImageFilter::Nearest: return VK_FILTER_NEAREST;
  case ImageFilter::Linear:  return VK_FILTER_LINEAR;
  }
  ERROR("invalid ImageFilter ", (u32)x, "\n");
  return VK_FILTER_NEAREST;
}

/* ----------------------------------------------------------------------------
 */
static VkSamplerAddressMode imageAddressModeToVulkan(ImageAddressMode x)
{
  switch(x)
  {
  case ImageAddressMode::Repeat:
    return VK_SAMPLER_ADDRESS_MODE_REPEAT;
  case ImageAddressMode::MirroredRepeat:
    return VK_SAMPLER_ADDRESS_MODE_MIRRORED_REPEAT;
  case ImageAddressMode::ClampToEdge:
    return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE;
  case ImageAddressMode::ClampToWhite:
    return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER;
  case ImageAddressMode::ClampToBlack:
    return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER;
  case ImageAddressMode::ClampToTransparent:
    return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER;
  }
  ERROR("invalid ImageAddressMode ", (u32)x, "\n");
  return VK_SAMPLER_ADDRESS_MODE_REPEAT;
}

/* ----------------------------------------------------------------------------
 */
static VkShaderStageFlagBits shaderKindToVulkan(ShaderKind kind)
{
  switch (kind)
  {
    case ShaderKind::Vertex:   return VK_SHADER_STAGE_VERTEX_BIT;
    case ShaderKind::Fragment: return VK_SHADER_STAGE_FRAGMENT_BIT;
    case ShaderKind::Compute:  return VK_SHADER_STAGE_COMPUTE_BIT;
    case ShaderKind::Geometry: return VK_SHADER_STAGE_GEOMETRY_BIT;
    default: return VK_SHADER_STAGE_FLAG_BITS_MAX_ENUM;
  }
}

/* ----------------------------------------------------------------------------
 */
static VkDescriptorType descriptorKindToVulkan(DescriptorKind kind)
{
  switch (kind)
  {
$ local function map(x, y)
  case DescriptorKind::$(x):
    return VK_DESCRIPTOR_TYPE_$(y);
$ end
  @map(UniformBuffer, UNIFORM_BUFFER)
  @map(CombinedImageSampler, COMBINED_IMAGE_SAMPLER)
  default:
    ERROR("invalid descriptor kind: ", (u32)kind, '\n');
    return {};
  }
}

/* ----------------------------------------------------------------------------
 */
Image Image::create(Renderer& renderer, const CreateParams& params)
{
  String name = resolved(params.debug_name, "unnamed"_str);

  DEBUG("creating gfx::Image '", name, "'\n");

  @getvk

$ local cerr = defcerr "gfx::Image"

  if (params.size.x == 0 || params.size.y == 0)
  {
    @cerr("cannot create a texture with 0 width or height");
    return nil;
  }

  if (params.usage.isNone())
  {
    @cerr("no usage flags set");
    return nil;
  }
  
  VkFormat format = imageFormatToVulkan(params.format);

  // This image will be transfered to.
  VkImageUsageFlags usage = VK_IMAGE_USAGE_TRANSFER_DST_BIT;
  
  if (params.usage.test(ImageUsage::Sampled))
    usage |= VK_IMAGE_USAGE_SAMPLED_BIT;
  if (params.usage.test(ImageUsage::ColorAttachment))
    usage |= VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
  
  VkImage image;
  if (!vk->createVkImage(
        &image,
        params.size,
        format,
        usage,
        name))
  {
    @cerr("failed to create VkImage");
    return nil;
  }

  DeviceHeapAllocation* ptr =
    vk->allocateAndBindVkImageMemory(image);

  if (ptr == nullptr)
  {
    @cerr("failed to allocate or bind VkImage memory");
    return nil;
  }

  auto failsafe_deallocate = deferWithCancel
  {
    vk->deallocate(ptr);
  };

  if (params.pixels != nullptr)
  {
    VkDeviceSize size = 
      params.size.x * params.size.y * 
      imageFormatToBytesPerPixel(params.format);

    if (!vk->stageVkImageMemory(
          params.pixels,
          size,
          image,
          params.size))
    {
      @cerr("failed to stage image memory");
      return nil;
    }
  }
  
  // Track the Image internally since we have to allocate device memory
  // for it.
  DeviceImage* internal_image = vk->image_pool.add();
  internal_image->handle = image;
  internal_image->ptr = ptr;

  return {vk->image_pool.indexOf(internal_image)};
}

/* ----------------------------------------------------------------------------
 */
void Image::destroy(Renderer& renderer)
{
  if (isnil(*this))
    return;

  @getvk

  DEBUG("destroying ", *this, '\n');

  DeviceImage* internal_image = vk->image_pool.get(index);
  if (internal_image == nullptr)
  {
    ERROR("attempt to destroy ", *this, " but its index has been lost\n");
    *this = nil;
    return;
  }

  vk->destroyVkImage(internal_image->handle);
  vk->deallocate(internal_image->ptr);

  vk->image_pool.remove(internal_image);
  *this = nil;
}

/* ----------------------------------------------------------------------------
 */
ImageView ImageView::create(Renderer& renderer, const CreateParams& params)
{
  String name = resolved(params.debug_name, "unnamed"_str);

  DEBUG("creating gfx::ImageView ", name, '\n');

$ local cerr = defcerr "gfx::Image"
  @getvk

  if (isnil(params.image))
  {
    @cerr("provided gfx::Image is nil");
    return nil;
  }

  DeviceImage* internal_image = vk->image_pool.get(params.image.index);
  if (internal_image == nullptr)
  {
    @cerr("index of provided gfx::Image is lost\n");
    return nil;
  }

  VkComponentMapping components = {};
  switch (params.format)
  {
  case ImageFormat::BW:
    components.r = VK_COMPONENT_SWIZZLE_R;
    components.g = VK_COMPONENT_SWIZZLE_R;
    components.b = VK_COMPONENT_SWIZZLE_R;
    components.a = VK_COMPONENT_SWIZZLE_R;
    break;

  case ImageFormat::BWA:
    components.r = VK_COMPONENT_SWIZZLE_R;
    components.g = VK_COMPONENT_SWIZZLE_R;
    components.b = VK_COMPONENT_SWIZZLE_R;
    components.a = VK_COMPONENT_SWIZZLE_A;
    break;

  case ImageFormat::RGB:
    components.r = VK_COMPONENT_SWIZZLE_R;
    components.g = VK_COMPONENT_SWIZZLE_G;
    components.b = VK_COMPONENT_SWIZZLE_B;
    components.a = VK_COMPONENT_SWIZZLE_ONE;
    break;
  }

  VkImageView view;
  if (!vk->createVkImageView(
        &view,
        internal_image->handle,
        imageFormatToVulkan(params.format),
        components,
        name))
  {
    @cerr("failed to create VkImageView");
    return nil;
  }

  return {(void*)view};
}

/* ----------------------------------------------------------------------------
 */
void ImageView::destroy(Renderer& renderer)
{
  if (isnil(*this))
    return;

  @getvk

  DEBUG("destroying ", *this, '\n');

  vk->destroyVkImageView((VkImageView)handle);
}

/* ----------------------------------------------------------------------------
 */
DescriptorSetLayout DescriptorSetLayout::create(
    Renderer& renderer,
    Slice<Binding> bindings,
    String debug_name)
{
  String name = resolved(debug_name, "unnamed"_str);
  
  DEBUG("creating gfx::DescriptorSetLayout '", name, "'\n");

  @getvk

$ local cerr = defcerr "gfx::DescriptorSetLayout"

  if (bindings.isEmpty())
  {
    @cerr("no bindings provided");
    return nil;
  }

  SmallArray<VkDescriptorSetLayoutBinding, 16> vk_bindings;

  for (Binding& binding : bindings)
  {
    vk_bindings.push(
    {
      .binding = binding.binding,
      .descriptorType = descriptorKindToVulkan(binding.kind),
      .descriptorCount = binding.count,
      .stageFlags = shaderKindToVulkan(binding.stage),
    });
  }

  VkDescriptorSetLayout layout;
  if (!vk->createVkDescriptorSetLayout(
        &layout,
        vk_bindings.asSlice(),
        name))
  {
    @cerr("failed to create VkDescriptorSetLayout");
    return nil;
  }

  return {(void*)layout};
}

/* ----------------------------------------------------------------------------
 */
void DescriptorSetLayout::destroy(Renderer& renderer)
{
  if (isnil(*this))
    return;

  DEBUG("destroying ", *this, '\n');

  @getvk

  vk->destroyVkDescriptorSetLayout((VkDescriptorSetLayout)handle);
  handle = nullptr;
}

/* ----------------------------------------------------------------------------
 */
DescriptorSet DescriptorSet::create(
    Renderer& renderer, 
    DescriptorSetLayout layout,
    String debug_name)
{
  String name = resolved(debug_name, "unnamed"_str);
  
  DEBUG("creating gfx::DescriptorSet '", name, "'\n");

  @getvk

$ local cerr = defcerr "gfx::DescriptorSet"

  if (isnil(layout))
  {
    @cerr("provided descriptor set layout is nil");
    return nil;
  }

  VkDescriptorSet set;
  if (!vk->allocateVkDescriptorSet(
        &set,
        &(VkDescriptorSetLayout&)layout.handle,
        name))
  {
    @cerr("failed to allocate VkDescriptorSet");
    return nil;
  }

  return {(void*)set};
}

/* ----------------------------------------------------------------------------
 */
void DescriptorSet::destroy(Renderer& renderer)
{
  if (isnil(*this))
    return;

  DEBUG("destroying ", *this, '\n');

  @getvk

  vk->deallocateVkDescriptorSet((VkDescriptorSet)handle);
  handle = nullptr;
}

/* ----------------------------------------------------------------------------
 */
void DescriptorSet::update(
    Renderer& renderer,
    u32 binding,
    u32 array_offset,
    Slice<ImageDescriptor> images)
{
  if (isnil(*this))
  {
    ERROR("attempt to update a nil gfx::DescriptorSetLayout\n");
    return;
  }

  @getvk

  SmallArray<VkDescriptorImageInfo, 16> infos;

  for (ImageDescriptor& image : images)
  {
    if (isnil(image.sampler))
    {
      ERROR("image descriptor contains nil gfx::Sampler\n");
      return;
    }

    if (isnil(image.view))
    {
      ERROR("image descriptor contains nil gfx::ImageView\n");
      return;
    }

    infos.push(
    {
      .sampler = (VkSampler)image.sampler.handle,
      .imageView = (VkImageView)image.view.handle,
      .imageLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL,
    });
  }

  vk->updateVkDescriptorSet_Image(
    (VkDescriptorSet)handle,
    array_offset,
    binding,
    infos.asSlice());
}

/* ----------------------------------------------------------------------------
 */
void DescriptorSet::update(
    Renderer& renderer,
    u32 binding,
    u32 array_offset,
    Slice<UniformBufferDescriptor> buffers)
{
  if (isnil(*this))
  {
    ERROR("attempt to update a nil DescriptorSetLayout\n");
    return;
  }

  @getvk

  SmallArray<VkDescriptorBufferInfo, 16> infos;

  for (UniformBufferDescriptor& write : buffers)
  {
    if (isnil(write.buffer))
    {
      ERROR("buffer write contains nil gfx::Buffer\n");
      return;
    }

    DeviceBuffer* internal_buffer = vk->buffer_pool.get(write.buffer.index);
    if (internal_buffer == nullptr)
    {
      ERROR("buffer write contains a gfx::Buffer who's index has been lost\n");
      return;
    }

    infos.push(
    {
      .buffer = internal_buffer->handle,
      .offset = write.offset,
      .range = write.range,
    });
  }

  vk->updateVkDescriptorSet_Buffer(
    (VkDescriptorSet)handle,
    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
    binding,
    array_offset,
    infos.asSlice());
}

/* ----------------------------------------------------------------------------
 */
Shader Shader::create(Renderer& renderer, const CreateParams& params)
{
  String name = resolved(params.debug_name, "unnamed"_str);

  DEBUG("creating shader '", name, "'\n");

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

$ local function cerr(...)
  ERROR("while creating gfx::Shader '", name, "': ", 
        $(cmn.joinArgs(',', ...)), '\n');
$ end

  if (params.spv_binary.isEmpty())
  {
    @cerr("passed an empty spv binary");
    return nil;
  }

  VkShaderModule shader;
  if (!vk->createVkShaderModule(
        &shader,
        params.spv_binary,
        name))
  {
    @cerr("failed to create VkShaderModule");
    return nil;
  }
  
  DeviceShader* dev_shader = vk->shader_pool.add();
  dev_shader->module = shader;
  dev_shader->stage = shaderKindToVulkan(params.kind);

  Shader result = {};
  result.index = vk->shader_pool.indexOf(dev_shader);

  return result;
}

/* ----------------------------------------------------------------------------
 */
void Shader::destroy(Renderer& renderer)
{
  if (isnil(*this))
    return;

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  DeviceShader* dev_shader = vk->shader_pool.get(index);
  if (dev_shader == nullptr)
  {
    ERROR("attempt to destroy ", *this, " but its index has been lost\n");
    *this = nil;
    return;
  }

  vk->destroyVkShaderModule(dev_shader->module);
  vk->shader_pool.remove(dev_shader);
  index = 0;
}

/* ----------------------------------------------------------------------------
 */
Pipeline Pipeline::create(Renderer& renderer, const CreateParams& params)
{
  String name = resolved(params.debug_name, "unnamed"_str);

  DEBUG("creating gfx::Pipeline '", name, "'\n");

  @getvk

$ local cerr = defcerr "gfx::Pipeline"

  if (isnil(params.vertex_shader))
  {
    @cerr("vertex shader not provided");
    return nil;
  }

  if (isnil(params.fragment_shader))
  {
    @cerr("fragment shader not provided");
    return nil;
  }

  SmallArray<VkPushConstantRange, 8> push_constant_ranges;
  for (const PushConstant& pc : params.layout.push_constant_ranges)
  {
    if (pc.size > vk->physical_device_properties.limits.maxPushConstantsSize)
    {
      @cerr("requested push constant size is greater than the selected "
            "device's maximum (", pc.size, " > ", 
            vk->physical_device_properties.limits.maxPushConstantsSize, ')');
      return nil;
    }

    push_constant_ranges.push(
    {
      .offset = pc.offset,
      .size = pc.size,
      .stageFlags = shaderKindToVulkan(pc.stages),
    });
  }

  // This could maybe be aliased but idk i dont wanna do that.
  SmallArray<VkDescriptorSetLayout, 8> descriptor_set_layouts;
  for (const DescriptorSetLayout& layout : 
       params.layout.descriptor_set_layouts)
  {
    descriptor_set_layouts.push((VkDescriptorSetLayout)layout.handle);
  }

  VkPipelineLayout layout;
  if (!vk->createVkPipelineLayout(
        &layout,
        descriptor_set_layouts.asSlice(),
        push_constant_ranges.asSlice(),
        name))
  {
    @cerr("failed to create VkPipelineLayout");
    return nil;
  }

  auto failsafe_destroy_layout = deferWithCancel
  {
    vk->destroyVkPipelineLayout(layout);
  };

  DeviceShader* vert_shader = vk->shader_pool.get(params.vertex_shader.index);
  if (vert_shader == nullptr)
  {
    @cerr("provided vertex shader ", params.vertex_shader, 
          " is not tracked in backend");
    return nil;
  }

  DeviceShader* frag_shader = 
    vk->shader_pool.get(params.fragment_shader.index);
  if (vert_shader == nullptr)
  {
    @cerr("provided fragment shader ", params.fragment_shader, 
          " is not tracked in backend");
    return nil;
  }

  VkPipeline pipeline;
  if (!vk->createVkPipeline(
        &pipeline,
        layout,
        vert_shader->module,
        frag_shader->module,
        params.has_vertex_input,
        name))
  {
    @cerr("failed to create VkPipeline");
    return nil;
  }

  DevicePipeline* dev_pipeline = vk->pipeline_pool.add();
  dev_pipeline->pipeline = pipeline;
  dev_pipeline->layout = layout;

  Pipeline result = {};
  result.index = vk->pipeline_pool.indexOf(dev_pipeline);

  failsafe_destroy_layout.cancel();

  return result;
}

/* ----------------------------------------------------------------------------
 */
Pipeline Pipeline::create(Renderer& renderer, const PipelineDef& def)
{
  assert(false);

  // CreateParams params = 
  // {
  //   .vertex_shader = def.vertex_shader->gfx_shader,
  //   .fragment_shader = def.fragment_shader->gfx_shader,
  //   .push_constant_size = def.push_constant_size,
  //   .has_vertex_input = def.has_vertex_input,
  //   .debug_name = def.name
  // };
  //
  // return create(renderer, params);
}

/* ----------------------------------------------------------------------------
 */
void Pipeline::destroy(Renderer& renderer)
{
  if (isnil(*this))
    return;

  Vulkan* vk = Vulkan::fromRendererInternal(renderer.internal);

  DevicePipeline* pipeline = vk->pipeline_pool.get(index);

  vk->destroyVkPipeline(pipeline->pipeline);
  vk->destroyVkPipelineLayout(pipeline->layout);

  index = 0;
  vk->pipeline_pool.remove(pipeline);
}

$ local function getvk()
  Vulkan* vk = Vulkan::fromRendererInternal(internal);
$ end

/* ----------------------------------------------------------------------------
 */
static VkClearValue colorToVkClearValue(Color color)
{
  VkClearValue clear_value = {};
  clear_value.color.uint32[0] = color.r;
  clear_value.color.uint32[1] = color.g;
  clear_value.color.uint32[2] = color.b;
  clear_value.color.uint32[3] = color.a;
  return clear_value;
}


/* ----------------------------------------------------------------------------
 */
b8 Renderer::beginRenderPass(
    vec2i render_area_pos,
    vec2u render_area_size,
    Image target,
    ImageView target_view,
    Color clear_color)
{
  if (isnil(target))
    return ERROR("attempt to begin a render pass with a nil target\n");

  if (isnil(target_view))
    return ERROR("attempt to begin a render pass with a nil target\n view");

  @getvk

  DeviceImage* internal_image = vk->image_pool.get(target.index);

  if (vk->state.command_buffer != VK_NULL_HANDLE)
    return ERROR("attempt to start a render pass while one is already "
                 "active\n");

  vk->state.command_buffer = vk->command_buffer;

  VkRect2D render_rect = 
  {
    .offset = 
    {
      .x = render_area_pos.x,
      .y = render_area_pos.y,
    },
    .extent = 
    {
      .width = render_area_size.x,
      .height = render_area_size.y,
    },
  };

  vk->cmdBeginRendering(
    vk->state.command_buffer,
    render_rect,
    internal_image->handle,
    (VkImageView)target_view.handle,
    colorToVkClearValue(clear_color));

  return true;
}

/* ----------------------------------------------------------------------------
 */
b8 Renderer::beginRenderPassOverSwapchainImage(
    vec2i render_area_pos,
    vec2u render_area_size,
    Color clear_color)
{
  @getvk

  if (vk->state.command_buffer != VK_NULL_HANDLE)
    return ERROR("attempt to start a render pass while one is already "
                 "active\n");

  vk->state.command_buffer = vk->command_buffer;

  VkRect2D render_rect = 
  {
    .offset = 
    {
      .x = render_area_pos.x,
      .y = render_area_pos.y,
    },
    .extent = 
    {
      .width = render_area_size.x,
      .height = render_area_size.y,
    },
  };

  u32 image_index;
  @vkc(vkAcquireNextImageKHR(
    vk->device,
    vk->swapchain,
    MAX_U64,
    VK_NULL_HANDLE,
    VK_NULL_HANDLE,
    &image_index))

  vk->cmdBeginRendering(
    vk->state.command_buffer,
    render_rect,
    vk->swapchain_buffers[image_index].image,
    vk->swapchain_buffers[image_index].view,
    colorToVkClearValue(clear_color));

  return true;
}

/* ----------------------------------------------------------------------------
 */
void Renderer::endRenderPass()
{
  @getvk
  if (vk->state.command_buffer == VK_NULL_HANDLE)
  {
    ERROR("attempt to end a render pass outside a render pass instance\n");
    return;
  }

  vk->cmdEndRendering(vk->state.command_buffer);
  vk->state.command_buffer = VK_NULL_HANDLE;
}

/* ----------------------------------------------------------------------------
 */
void Renderer::setScissor(vec2i pos, vec2u size)
{
  @getvk

  TRACE("setting scissor to ", pos, ' ', size, "\n");

  VkRect2D scissor = 
  {
    .offset = 
    {
      .x = pos.x,
      .y = pos.y,
    },
    .extent = 
    {
      .width = size.x,
      .height = size.y,
    }
  };

  vkCmdSetScissor(vk->command_buffer, 0, 1, &scissor);
}

/* ----------------------------------------------------------------------------
 */
void Renderer::setViewport(vec2f pos, vec2f size)
{
  @getvk

  TRACE("setting viewport to ", pos, ' ', size, '\n');

  VkViewport viewport = 
  {
    .x = pos.x,
    .y = pos.y,
    .width = size.x,
    .height = size.y,
    .minDepth = 0.f,
    .maxDepth = 1.f,
  };

  vkCmdSetViewport(vk->command_buffer, 0, 1, &viewport);
}

/* ----------------------------------------------------------------------------
 */
void Renderer::bindPipeline(Pipeline pipeline)
{
  if (isnil(pipeline))
  {
    ERROR("attempt to bind nil gfx::Pipeline\n");
    return;
  }

  @getvk
  DevicePipeline* dpipeline = vk->pipeline_pool.get(pipeline.index);

  if (dpipeline == vk->state.bound_pipeline)
    // Already bound.
    return;

  TRACE("binding ", pipeline, '\n');

  vkCmdBindPipeline(
    vk->command_buffer, 
    VK_PIPELINE_BIND_POINT_GRAPHICS,
    dpipeline->pipeline);
}

/* ----------------------------------------------------------------------------
 */
void Renderer::bindDescriptorSet(u32 set_idx, DescriptorSet set)
{
  if (isnil(set))
  {
    ERROR("attempt to bind nil gfx::DescriptorSet\n");
    return;
  }

  @getvk

  if (vk->state.bound_pipeline == nullptr)
  {
    ERROR("attempt to bind a gfx::DescriptorSet before binding a pipeline\n");
    return;
  }

  vkCmdBindDescriptorSets(
    vk->state.command_buffer,
    VK_PIPELINE_BIND_POINT_GRAPHICS,
    vk->state.bound_pipeline->layout,
    set_idx,
    1,
    &(VkDescriptorSet&)set.handle,
    0,
    nullptr);
}

}
